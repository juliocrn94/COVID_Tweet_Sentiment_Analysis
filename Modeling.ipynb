{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DEPENDENCIES\n",
    "\n",
    "## <span style=\"color:red\"> MUST RUN ALWAYS</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliorenteria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Checkpoints\n",
    "import pickle\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Preprocessing\n",
    "import re    # RegEx for removing non-letter characters\n",
    "import nltk  #natural language processing\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.preprocessing as pr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = './data/airline_tweets.csv'\n",
    "#file_name = './data/covid_tweets.csv'\n",
    "file_name = './data/generic_tweets.csv'\n",
    "\n",
    "data = pd.read_csv(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asinging a random number to each observation for future sampling\n",
    "np.random.seed(11)\n",
    "data['rand'] = np.random.random_sample(size=(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks for pointing out the crucial problems @...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.180270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please ignore cheesey music</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.019475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just got home from a meeting with the girls......</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>victory for the bulldogs was celebrated by 3 w...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.724934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://tinyurl.com/ateltl &amp;lt;-- Rocksteady pt II</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment      rand\n",
       "0  Thanks for pointing out the crucial problems @...          1  0.180270\n",
       "1                        please ignore cheesey music         -1  0.019475\n",
       "2  just got home from a meeting with the girls......          0  0.463219\n",
       "3  victory for the bulldogs was celebrated by 3 w...         -1  0.724934\n",
       "4  http://tinyurl.com/ateltl &lt;-- Rocksteady pt II          0  0.420204"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVA - Total:  8830 , Proportion:  1.0\n",
      "NEUTRALES - Total:  12668 , Proportion:  0.6970318913798548\n",
      "POSITIVE - Total:  9897 , Proportion:  0.8921895523896131\n"
     ]
    }
   ],
   "source": [
    "# Sampling the min ammount from the other to to obtain a balanced dataset\n",
    "\n",
    "samp_neg = min(data.sentiment.value_counts())/data.sentiment.value_counts()[-1]\n",
    "samp_neu = min(data.sentiment.value_counts())/data.sentiment.value_counts()[0]\n",
    "samp_pos = min(data.sentiment.value_counts())/data.sentiment.value_counts()[1]\n",
    "\n",
    "print('NEGATIVA - Total: ',data.sentiment.value_counts()[-1], ', Proportion: ',samp_neg)\n",
    "print('NEUTRALES - Total: ',data.sentiment.value_counts()[0], ', Proportion: ',samp_neu)\n",
    "print('POSITIVE - Total: ',data.sentiment.value_counts()[1], ', Proportion: ',samp_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    8865\n",
      " 1    8855\n",
      "-1    8830\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>please ignore cheesey music</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.019475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>victory for the bulldogs was celebrated by 3 w...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.724934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@MrWize sike sike call it truce???????????????...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.344624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting for an email that will probably never ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.318799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@chaoscartel That is annoying. What gear is it...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.055674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment      rand\n",
       "0                        please ignore cheesey music         -1  0.019475\n",
       "1  victory for the bulldogs was celebrated by 3 w...         -1  0.724934\n",
       "2  @MrWize sike sike call it truce???????????????...         -1  0.344624\n",
       "3  Waiting for an email that will probably never ...         -1  0.318799\n",
       "4  @chaoscartel That is annoying. What gear is it...         -1  0.055674"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the sampled datasets for Negative, Neutral and Positive\n",
    "frames = [data.loc[(data['sentiment'] == -1) & (data['rand'] <= samp_neg)],\n",
    "data.loc[(data['sentiment'] == 0) & (data['rand'] <= samp_neu)],\n",
    "data.loc[(data['sentiment'] == 1) & (data['rand'] <= samp_pos)]]\n",
    "\n",
    "data = pd.concat(frames)\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.sentiment.value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZING\n",
    "\n",
    "Separate tweets into lists of word (or components) that carry meanings.\n",
    "\n",
    "Removing/converting the following elements:\n",
    "- Uppercase\n",
    "- Urls\n",
    "- Simbols\n",
    "- Numbers\n",
    "- English Stopwords\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format, tokenize and remove stopwords from tweets.\n",
    "def tweet_to_words(tweet):\n",
    "    ''' Convert tweet text into a sequence of words '''\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = tweet.lower()\n",
    "    # remove tweeter users and hashtags ( @xxx, #xxx )\n",
    "    text = re.sub(r\"[@#]\\w+\", \" \", text)\n",
    "    # remove https\n",
    "    text = re.sub(r\"[(http(s)?):\\/\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)\", \" \", text)\n",
    "    #text = \" \".join(filter(lambda w: w.find(\"https://\") == -1, text.split(\" \")))\n",
    "    # remove non letters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    #remove numbers\n",
    "    text = re.sub(r\"[0-9]*\", \"\", text)\n",
    "    # tokenize\n",
    "    words = text.split()\n",
    "    # remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    # apply stemming\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    # return list\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL TWEET ->\n",
      " FJGKFLD;'Sdh WHY AM I NOT AT HOMETOWN DAYS WITH MY FRIENDS.\n",
      "\n",
      "PROCESSED TWEET -> ['fjgkfld', 'sdh', 'hometown', 'day', 'friend']\n",
      "\\SENTIMENT ->  0\n"
     ]
    }
   ],
   "source": [
    "# Testing tweet_to_words Function\n",
    "sel = 16003\n",
    "print(\"\\nORIGINAL TWEET ->\\n\", data['text'][sel])\n",
    "print(\"\\nPROCESSED TWEET ->\", tweet_to_words(data['text'][sel]))\n",
    "print(\"\\SENTIMENT -> \", data['sentiment'][sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc50f7968934a0eb4ed06c55d2d4b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26550.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to each tweet and store as X\n",
    "X = []\n",
    "for i in trange(len(data['text'])):\n",
    "    X.append(tweet_to_words(data['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(data['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to each tweet and store as X\n",
    "# X = list(map(tweet_to_words, data['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš© CHECKPOINT \n",
    "\n",
    "Saving **Tokenized List** as a Pickle File to retrieve latter and save memory and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving X to a PICKLE to retrieve back latter:\n",
    "\n",
    "pickle_dic = {'df_1245.csv':'1245_',\n",
    "              'airline_tweets.csv':'airlines_',\n",
    "              'covid_tweets.csv':'covid_',\n",
    "              'generic_tweets.csv':'generic_'}\n",
    "\n",
    "# Code to Save PICKLE\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'X.pkl', 'wb') as f:\n",
    "#    pickle.dump(X, f)\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'Y.pkl', 'wb') as f:\n",
    "#    pickle.dump(Y, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Retrieve PICKLE\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'X.pkl', 'rb') as f:\n",
    "#    X = pickle.load(f)\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'Y.pkl', 'rb') as f:\n",
    "#    Y = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in the total set :    26550\n",
      "Number of tweets in the training set : 18585\n",
      "Number of tweets in the testing set :  7965\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=13)\n",
    "\n",
    "print('Number of tweets in the total set :    {}'.format(len(X)))\n",
    "print('Number of tweets in the training set : {}'.format(len(X_train)))\n",
    "print('Number of tweets in the testing set :  {}'.format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DISTRIBUTION -  {0: 6143, 1: 6208, 2: 6234}\n",
      "TESTING DISTRIBUTION -  {0: 2687, 1: 2657, 2: 2621}\n"
     ]
    }
   ],
   "source": [
    "# Counting the distribution in training and test datasets\n",
    "unique_tr, counts_tr = np.unique(y_train, return_counts=True)\n",
    "print('TRAINING DISTRIBUTION - ',dict(zip(unique_tr, counts_tr)))\n",
    "\n",
    "unique_te, counts_te = np.unique(y_test, return_counts=True)\n",
    "print('TESTING DISTRIBUTION - ',dict(zip(unique_te, counts_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "                ngram_range=(1, 1),\n",
       "                preprocessor=<function <lambda> at 0x7f8708316510>,\n",
       "                stop_words=None, strip_accents=None,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function <lambda> at 0x7f87083162f0>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "vocabulary_size = 1000    # Rounded up from 17422\n",
    "\n",
    "# Generating Bag of Words\n",
    "# Tweets have already been preprocessed hence dummy function will be passed in to preprocessor & tokenizer step\n",
    "count_vector = CountVectorizer(max_features=vocabulary_size, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "count_vector.fit(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT VECTORIZER\n",
      "VOCABULARY SIZE:  1000\n",
      "VOCABULARY CONTENT:  {'like': 499, 'person': 638, 'right': 715, 'keep': 465, 'fall': 293, 'asleep': 49, 'suppos': 835, 'awak': 53, 'miss': 559, 'way': 941, 'school': 735, 'back': 61, 'later': 483, 'forget': 331, 'someth': 786, 'morn': 571, 'ran': 692, 'warm': 935, 'shop': 760, 'x': 980, 'happi': 391, 'good': 363, 'day': 214, 'fuck': 342, 'hate': 393, 'run': 722, 'special': 802, 'k': 464, 'garden': 348, 'thank': 857, 'super': 833, 'laptop': 480, 'screen': 736, 'excit': 280, 'week': 947, 'u': 905, 'go': 357, 'away': 54, 'stand': 807, 'next': 590, 'start': 810, 'video': 923, 'edit': 256, 'first': 319, 'movi': 574, 'ad': 6, 'updat': 910, 'know': 475, 'work': 969, 'pay': 631, 'though': 862, 'mother': 572, 'total': 882, 'finish': 317, 'til': 869, 'bad': 62, 'news': 589, 'graduat': 370, 'think': 860, 'im': 437, 'feel': 304, 'well': 951, 'tire': 873, 'scare': 733, 'cuz': 203, 'wake': 930, 'sick': 767, 'lose': 515, 'follow': 328, 'wonder': 965, 'end': 262, 'drive': 244, 'even': 269, 'anyth': 38, 'stupid': 825, 'doubl': 238, 'wish': 960, 'twit': 901, 'much': 576, 'post': 660, 'interest': 444, 'oh': 606, 'mood': 569, 'home': 415, 'w': 928, 'birthday': 89, 'sooo': 793, 'hug': 425, 'upset': 912, 'may': 539, 'amp': 30, 'text': 855, 'messag': 548, 'month': 568, 'black': 92, 'lol': 509, 'time': 871, 'went': 952, 'fast': 299, 'left': 491, 'hope': 417, 'move': 573, 'soon': 792, 'co': 167, 'one': 612, 'said': 726, 'pic': 641, 'made': 527, 'hand': 387, 'get': 350, 'weekend': 948, 'finger': 316, 'cross': 198, 'let': 495, 'freak': 335, 'key': 466, 'nope': 596, 'lost': 516, 'found': 334, 'ahhh': 16, 'ok': 608, 'rememb': 707, 'film': 312, 'never': 587, 'saw': 731, 'check': 151, 'hour': 421, 'old': 610, 'least': 489, 'coffe': 169, 'peopl': 635, 'today': 874, 'mom': 562, 'look': 514, 'exactli': 276, 'guitar': 377, 'took': 880, 'guy': 379, 'three': 864, 'show': 763, 'shit': 757, 'realli': 700, 'need': 586, 'add': 7, 'la': 477, 'fit': 321, 'busi': 126, 'could': 186, 'sister': 774, 'meet': 543, 'pleas': 652, 'tonight': 879, 'probabl': 670, 'hey': 406, 'haha': 382, 'stop': 816, 'see': 742, 'date': 211, 'still': 814, 'want': 933, 'pictur': 643, 'girl': 354, 'world': 970, 'lt': 519, 'listen': 504, 'sweet': 838, 'talk': 845, 'cute': 202, 'take': 842, 'bore': 103, 'cant': 136, 'wait': 929, 'year': 990, 'stress': 821, 'stay': 812, 'got': 368, 'wear': 942, 'green': 372, 'shirt': 756, 'eye': 286, 'hear': 398, 'hang': 388, 'final': 313, 'luck': 520, 'jona': 459, 'night': 592, 'come': 172, 'love': 518, 'make': 530, 'awww': 57, 'talent': 844, 'woke': 963, 'dream': 241, 'sleep': 777, 'prob': 669, 'note': 598, 'self': 745, 'hous': 422, 'fun': 344, 'guess': 376, 'true': 894, 'done': 235, 'sun': 829, 'shine': 755, 'safe': 725, 'say': 732, 'alon': 21, 'suck': 826, 'goe': 359, 'degre': 220, 'someon': 785, 'sit': 775, 'hahaha': 384, 'omg': 611, 'hr': 423, 'play': 651, 'dead': 215, 'friend': 340, 'help': 405, 'almost': 20, 'boy': 109, 'drink': 243, 'wanna': 932, 'open': 615, 'broken': 117, 'weird': 949, 'app': 41, 'pretti': 666, 'cool': 184, 'great': 371, 'ladi': 478, 'tweet': 899, 'impress': 439, 'eat': 255, 'sinc': 771, 'last': 481, 'thought': 863, 'slept': 779, 'glad': 356, 'sure': 836, 'em': 260, 'cri': 197, 'town': 885, 'two': 903, 'dinner': 228, 'dark': 210, 'chocol': 159, 'twitter': 902, 'number': 601, 'bit': 90, 'voic': 926, 'heat': 401, 'noth': 599, 'sigh': 769, 'alway': 28, 'disappoint': 230, 'everyon': 274, 'might': 551, 'els': 259, 'bring': 114, 'tho': 861, 'late': 482, 'man': 532, 'connect': 181, 'game': 347, 'bag': 63, 'gotta': 369, 'search': 738, 'ill': 436, 'call': 131, 'live': 506, 'dont': 236, 'till': 870, 'gone': 361, 'mind': 556, 'watch': 939, 'ate': 51, 'cours': 189, 'record': 702, 'decid': 218, 'die': 226, 'friday': 339, 'sad': 723, 'e': 250, 'sunshin': 832, 'better': 83, 'part': 627, 'shut': 765, 'comput': 176, 'readi': 696, 'bummer': 124, 'exam': 277, 'tom': 877, 'top': 881, 'kinda': 471, 'actual': 5, 'leav': 490, 'colleg': 171, 'ye': 987, 'tri': 891, 'gonna': 362, 'ticket': 868, 'vega': 919, 'summer': 828, 'anyon': 37, 'googl': 366, 'unfortun': 909, 'abl': 0, 'nd': 584, 'gig': 353, 'everyth': 275, 'tv': 898, 'ah': 14, 'idea': 433, 'awesom': 55, 'hair': 385, 'cut': 201, 'homework': 416, 'fri': 338, 'chicken': 154, 'everi': 272, 'song': 790, 'new': 588, 'soo': 791, 'long': 512, 'poor': 657, 'crazi': 195, 'big': 85, 'babi': 60, 'kind': 470, 'puppi': 681, 'knew': 474, 'lil': 500, 'set': 750, 'quot': 686, 'nobodi': 594, 'lunch': 522, 'alreadi': 24, 'ur': 913, 'repli': 709, 'problem': 671, 'sorri': 797, 'forward': 333, 'share': 753, 'breath': 113, 'okay': 609, 'invit': 447, 'mine': 557, 'promis': 676, 'congratul': 180, 'five': 322, 'rock': 718, 'ahh': 15, 'parti': 628, 'wolverin': 964, 'seem': 743, 'jealou': 454, 'john': 456, 'definit': 219, 'yea': 988, 'r': 687, 'kid': 468, 'best': 81, 'xd': 981, 'bed': 77, 'beauti': 75, 'tomorrow': 878, 'sunday': 830, 'enjoy': 264, 'parent': 625, 'revis': 713, 'littl': 505, 'apart': 40, 'id': 432, 'use': 915, 'whole': 956, 'hun': 428, 'french': 337, 'earli': 252, 'tell': 852, 'match': 535, 'without': 962, 'copi': 185, 'yesterday': 992, 'although': 27, 'read': 695, 'yet': 993, 'yr': 997, 'told': 876, 'clean': 163, 'present': 665, 'ever': 271, 'wow': 976, 'book': 102, 'n': 581, 'stuff': 824, 'buy': 127, 'nap': 583, 'phone': 639, 'convers': 182, 'cat': 141, 'walk': 931, 'block': 96, 'name': 582, 'bloodi': 98, 'give': 355, 'boyfriend': 110, 'us': 914, 'air': 18, 'red': 704, 'happen': 390, 'lucki': 521, 'xxx': 984, 'put': 682, 'forgot': 332, 'min': 555, 'b': 58, 'singl': 773, 'amaz': 29, 'ask': 48, 'th': 856, 'holiday': 414, 'rest': 710, 'rain': 690, 'hi': 407, 'quit': 685, 'weather': 943, 'nite': 593, 'dude': 247, 'chillin': 157, 'mean': 541, 'bu': 121, 'train': 888, 'delay': 221, 'catch': 142, 'gut': 378, 'came': 132, 'ear': 251, 'hurt': 430, 'nice': 591, 'iphon': 448, 'bike': 86, 'visit': 925, 'extra': 285, 'traffic': 887, 'welcom': 950, 'would': 975, 'save': 730, 'saturday': 729, 'blast': 94, 'must': 579, 'xx': 983, 'sore': 796, 'hilari': 409, 'ya': 985, 'goodnight': 365, 'lot': 517, 'wont': 966, 'stomach': 815, 'pain': 623, 'fail': 291, 'download': 239, 'answer': 34, 'fan': 296, 'question': 683, 'hospit': 419, 'dunno': 249, 'teach': 850, 'especi': 267, 'fire': 318, 'swine': 841, 'flu': 327, 'pray': 664, 'travel': 889, 'ignor': 435, 'meant': 542, 'direct': 229, 'cold': 170, 'fli': 324, 'headach': 396, 'pass': 629, 'place': 648, 'wtf': 979, 'lesson': 494, 'sing': 772, 'gt': 375, 'group': 373, 'either': 258, 'mention': 546, 'believ': 80, 'kill': 469, 'yeah': 989, 'near': 585, 'wrong': 978, 'hit': 410, 'car': 137, 'music': 578, 'tast': 847, 'feet': 306, 'cake': 130, 'momma': 564, 'crap': 193, 'cover': 191, 'tour': 884, 'heard': 399, 'memori': 544, 'card': 138, 'realiz': 699, 'c': 129, 'race': 688, 'sometim': 787, 'life': 497, 'absolut': 1, 'congrat': 179, 'agre': 13, 'son': 789, 'worst': 973, 'depress': 222, 'p': 619, 'ago': 12, 'mum': 577, 'album': 19, 'yay': 986, 'find': 314, 'thing': 859, 'white': 955, 'remind': 708, 'luv': 523, 'front': 341, 'behind': 79, 'box': 108, 'horribl': 418, 'free': 336, 'boss': 104, 'return': 711, 'bug': 122, 'product': 672, 'pack': 620, 'cloth': 165, 'pink': 645, 'lay': 486, 'pool': 656, 'photo': 640, 'money': 567, 'tip': 872, 'etc': 268, 'church': 160, 'public': 679, 'dear': 217, 'bird': 88, 'flower': 326, 'perfect': 636, 'heart': 400, 'citi': 161, 'camera': 133, 'also': 26, 'store': 817, 'vote': 927, 'send': 746, 'food': 329, 'june': 463, 'hell': 403, 'grow': 374, 'anim': 31, 'view': 924, 'site': 776, 'real': 697, 'blog': 97, 'list': 503, 'hehe': 402, 'ice': 431, 'cream': 196, 'pick': 642, 'caus': 144, 'celebr': 146, 'job': 455, 'sort': 798, 'onlin': 613, 'figur': 310, 'ugh': 906, 'possibl': 659, 'wash': 936, 'short': 761, 'tea': 849, 'differ': 227, 'chang': 148, 'link': 502, 'fell': 307, 'coupl': 188, 'shall': 751, 'notic': 600, 'class': 162, 'gift': 352, 'dad': 205, 'cousin': 190, 'shot': 762, 'hangov': 389, 'mad': 526, 'hungri': 429, 'machin': 525, 'shame': 752, 'face': 288, 'full': 343, 'aww': 56, 'she': 754, 'btw': 120, 'bank': 66, 'monday': 566, 'bath': 69, 'lazi': 487, 'fill': 311, 'broke': 116, 'fix': 323, 'afternoon': 10, 'terribl': 853, 'point': 655, 'sunni': 831, 'recov': 703, 'window': 959, 'half': 386, 'damn': 207, 'pull': 680, 'sat': 728, 'outsid': 618, 'care': 139, 'page': 621, 'rip': 716, 'mama': 531, 'profil': 673, 'order': 616, 'blackberri': 93, 'arriv': 47, 'concert': 177, 'sign': 770, 'god': 358, 'studi': 823, 'sleepi': 778, 'screw': 737, 'head': 395, 'famili': 295, 'swim': 840, 'plan': 649, 'annoy': 32, 'ju': 460, 'ppl': 662, 'chines': 158, 'drunk': 246, 'due': 248, 'funni': 345, 'count': 187, 'sent': 747, 'beat': 74, 'doesnt': 233, 'huge': 426, 'test': 854, 'stuck': 822, 'second': 741, 'park': 626, 'whatev': 954, 'charg': 149, 'wors': 972, 'math': 537, 'ipod': 449, 'yum': 998, 'seen': 744, 'code': 168, 'plu': 653, 'far': 298, 'tweetdeck': 900, 'turn': 897, 'peac': 633, 'si': 766, 'facebook': 289, 'dog': 234, 'shower': 764, 'straight': 819, 'laugh': 484, 'hmm': 411, 'anyway': 39, 'hard': 392, 'beach': 73, 'cancel': 134, 'mani': 534, 'internet': 445, 'jump': 462, 'bare': 68, 'everybodi': 273, 'write': 977, 'interview': 446, 'blue': 99, 'expens': 283, 'past': 630, 'moment': 563, 'vid': 922, 'side': 768, 'gave': 349, 'brother': 118, 'relax': 705, 'age': 11, 'star': 808, 'trek': 890, 'men': 545, 'perform': 637, 'snl': 783, 'ooh': 614, 'fav': 300, 'brought': 119, 'becom': 76, 'prom': 675, 'high': 408, 'middl': 550, 'danc': 208, 'worth': 974, 'close': 164, 'miley': 553, 'bday': 72, 'lone': 511, 'soooo': 794, 'serious': 748, 'bodi': 100, 'huh': 427, 'wed': 946, 'trip': 892, 'offici': 605, 'that': 858, 'case': 140, 'sound': 799, 'episod': 266, 'join': 457, 'ha': 381, 'around': 46, 'bill': 87, 'st': 806, 'beer': 78, 'magic': 528, 'stick': 813, 'sale': 727, 'war': 934, 'ouch': 617, 'along': 22, 'boo': 101, 'doctor': 232, 'pm': 654, 'power': 661, 'load': 508, 'tuesday': 895, 'ass': 50, 'usual': 916, 'mr': 575, 'cheer': 152, 'taylor': 848, 'track': 886, 'enough': 265, 'pc': 632, 'fantast': 297, 'email': 261, 'insid': 442, 'room': 719, 'woo': 967, 'hot': 420, 'minut': 558, 'fine': 315, 'bye': 128, 'seat': 740, 'lmao': 507, 'hold': 413, 'light': 498, 'piec': 644, 'paper': 624, 'road': 717, 'f': 287, 'door': 237, 'spell': 803, 'ach': 3, 'dress': 242, 'reason': 701, 'youtub': 996, 'mommi': 565, 'moon': 570, 'instead': 443, 'upload': 911, 'crash': 194, 'didnt': 225, 'mmm': 560, 'offic': 604, 'yummi': 999, 'kiss': 472, 'dang': 209, 'line': 501, 'ride': 714, 'manag': 533, 'anymor': 36, 'earlier': 253, 'g': 346, 'suggest': 827, 'ps': 678, 'mayb': 540, 'stori': 818, 'version': 920, 'forev': 330, 'bowl': 107, 'pizza': 647, 'breakfast': 112, 'servic': 749, 'strang': 820, 'tan': 846, 'rd': 694, 'longer': 513, 'sadli': 724, 'gym': 380, 'excel': 278, 'anoth': 33, 'support': 834, 'less': 493, 'experi': 284, 'season': 739, 'cannot': 135, 'addict': 8, 'bought': 105, 'worri': 971, 'flight': 325, 'peep': 634, 'young': 995, 'troubl': 893, 'law': 485, 'spend': 804, 'da': 204, 'alot': 23, 'word': 968, 'compani': 174, 'break': 111, 'david': 213, 'cook': 183, 'fish': 320, 'wife': 957, 'babe': 59, 'smile': 782, 'jay': 453, 'project': 674, 'releas': 706, 'fact': 290, 'comment': 173, 'felt': 308, 'event': 270, 'juli': 461, 'yep': 991, 'fb': 303, 'hubbi': 424, 'win': 958, 'websit': 945, 'idk': 434, 'myspac': 580, 'fam': 294, 'mac': 524, 'mess': 547, 'lie': 496, 'issu': 451, 'aw': 52, 'bout': 106, 'easi': 254, 'favourit': 302, 'except': 279, 'bless': 95, 'bro': 115, 'proud': 677, 'spent': 805, 'bar': 67, 'appar': 42, 'inde': 441, 'hello': 404, 'water': 940, 'chat': 150, 'bitch': 91, 'met': 549, 'small': 781, 'london': 510, 'thursday': 866, 'kitti': 473, 'v': 917, 'what': 953, 'ball': 64, 'tummi': 896, 'leg': 492, 'taken': 843, 'fight': 309, 'includ': 440, 'offer': 603, 'bum': 123, 'mail': 529, 'piss': 646, 'mate': 536, 'vacat': 918, 'paid': 622, 'band': 65, 'deserv': 223, 'sweeti': 839, 'hahah': 383, 'cd': 145, 'english': 263, 'fair': 292, 'sooooo': 795, 'complet': 175, 'healthi': 397, 'practic': 663, 'cup': 199, 'burn': 125, 'understand': 908, 'pop': 658, 'bgt': 84, 'uk': 907, 'afraid': 9, 'joke': 458, 'account': 2, 'throat': 865, 'daughter': 212, 'somewher': 788, 'goin': 360, 'hmmm': 412, 'appreci': 44, 'drag': 240, 'anybodi': 35, 'realis': 698, 'teacher': 851, 'isnt': 450, 'deal': 216, 'dm': 231, 'ruin': 721, 'drop': 245, 'scari': 734, 'space': 800, 'via': 921, 'none': 595, 'bbq': 70, 'confus': 178, 'rather': 693, 'normal': 597, 'gorgeou': 367, 'touch': 883, 'starbuck': 809, 'gettin': 351, 'favorit': 301, 'print': 668, 'caught': 143, 'ive': 452, 'raini': 691, 'mobil': 561, 'imagin': 438, 'chanc': 147, 'xoxo': 982, 'surpris': 837, 'radio': 689, 'across': 4, 'wast': 937, 'shoe': 758, 'chees': 153, 'mile': 552, 'l': 476, 'lame': 479, 'review': 712, 'arm': 45, 'milk': 554, 'round': 720, 'feelin': 305, 'yo': 994, 'shoot': 759, 'wit': 961, 'crack': 192, 'design': 224, 'thx': 867, 'quiet': 684, 'slow': 780, 'current': 200, 'web': 944, 'club': 166, 'state': 811, 'learn': 488, 'children': 155, 'exhaust': 281, 'togeth': 875, 'price': 667, 'matter': 538, 'alright': 25, 'goodby': 364, 'bc': 71, 'appl': 43, 'daddi': 206, 'havent': 394, 'nyc': 602, 'sold': 784, 'bet': 82, 'kick': 467, 'eh': 257, 'chill': 156, 'speak': 801, 'expect': 282, 'wat': 938, 'type': 904, 'ohh': 607, 'plane': 650, 'aint': 17}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer creates a vocabulary. Checking Vocabulary\n",
    "print('COUNT VECTORIZER')\n",
    "print('VOCABULARY SIZE: ', len(count_vector.vocabulary_))\n",
    "print('VOCABULARY CONTENT: ', count_vector.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# characters level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(max_df=0.90, min_df=2, analyzer='word', preprocessor=lambda x: x, tokenizer=lambda x: x,\n",
    "                                   ngram_range=(1,2), max_features=1000)\n",
    "tfidf_vect_ngram.fit(X_train)\n",
    "\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train) \n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF VECTORIZER\n",
      "VOCABULARY SIZE:  1000\n",
      "VOCABULARY CONTENT:  {'like': 500, 'person': 646, 'right': 719, 'keep': 464, 'fall': 272, 'asleep': 45, 'suppos': 830, 'awak': 49, 'miss': 565, 'way': 939, 'school': 736, 'back': 57, 'later': 483, 'forget': 306, 'someth': 784, 'morn': 576, 'ran': 694, 'warm': 932, 'shop': 758, 'x': 980, 'happi': 390, 'good': 354, 'day': 201, 'good day': 355, 'fuck': 318, 'hate': 395, 'run': 725, 'special': 799, 'k': 463, 'garden': 324, 'thank': 848, 'super': 828, 'laptop': 478, 'screen': 737, 'excit': 259, 'week': 945, 'u': 897, 'go': 337, 'away': 50, 'stand': 803, 'next': 599, 'start': 807, 'video': 914, 'first': 296, 'movi': 580, 'ad': 4, 'updat': 902, 'know': 473, 'work': 966, 'pay': 641, 'though': 856, 'mother': 577, 'happi mother': 392, 'mother day': 578, 'total': 876, 'finish': 295, 'til': 863, 'bad': 60, 'news': 598, 'graduat': 368, 'think': 854, 'im': 438, 'feel': 279, 'well': 949, 'tire': 867, 'scare': 735, 'cuz': 192, 'wake': 923, 'sick': 765, 'lose': 521, 'follow': 303, 'wonder': 962, 'end': 242, 'drive': 228, 'even': 249, 'anyth': 34, 'stupid': 820, 'doubl': 223, 'wish': 958, 'twit': 894, 'much': 582, 'post': 666, 'interest': 443, 'oh': 614, 'mood': 575, 'home': 414, 'w': 919, 'birthday': 84, 'go home': 342, 'sooo': 791, 'hug': 425, 'upset': 904, 'may': 544, 'amp': 28, 'text': 846, 'messag': 554, 'month': 574, 'black': 87, 'lol': 511, 'time': 865, 'went': 950, 'fast': 277, 'left': 491, 'hope': 416, 'move': 579, 'soon': 790, 'co': 155, 'one': 621, 'said': 729, 'pic': 649, 'made': 532, 'hand': 387, 'get': 326, 'weekend': 946, 'finger': 294, 'cross': 187, 'let': 495, 'get back': 327, 'let know': 496, 'freak': 310, 'key': 465, 'nope': 606, 'lost': 522, 'found': 309, 'ahhh': 14, 'ok': 617, 'rememb': 711, 'film': 290, 'never': 596, 'saw': 733, 'check': 140, 'hour': 421, 'old': 619, 'least': 489, 'coffe': 157, 'go work': 346, 'peopl': 643, 'today': 868, 'mom': 568, 'day mom': 204, 'look': 517, 'exactli': 256, 'guitar': 377, 'took': 874, 'guy': 379, 'three': 858, 'show': 761, 'shit': 756, 'realli': 701, 'need': 594, 'add': 5, 'realli need': 703, 'la': 475, 'fit': 298, 'busi': 116, 'could': 175, 'wish could': 959, 'next week': 601, 'sister': 772, 'meet': 549, 'pleas': 658, 'tonight': 873, 'probabl': 674, 'hey': 407, 'haha': 382, 'stop': 813, 'see': 741, 'date': 199, 'still': 811, 'want': 927, 'pictur': 651, 'good time': 360, 'girl': 334, 'world': 969, 'lt': 525, 'listen': 506, 'sweet': 833, 'talk': 840, 'cute': 191, 'take': 837, 'bore': 97, 'cant': 126, 'wait': 920, 'year': 990, 'stress': 816, 'stay': 809, 'cant wait': 127, 'got': 364, 'got home': 366, 'wear': 940, 'green': 372, 'shirt': 755, 'eye': 265, 'hear': 399, 'hang': 388, 'final': 291, 'luck': 526, 'good luck': 356, 'jona': 458, 'night': 603, 'come': 160, 'love': 524, 'make': 534, 'next time': 600, 'awww': 53, 'talent': 839, 'woke': 961, 'dream': 225, 'sleep': 775, 'prob': 673, 'note': 608, 'self': 746, 'hous': 422, 'fun': 320, 'guess': 376, 'true': 889, 'done': 221, 'sun': 824, 'safe': 728, 'say': 734, 'alon': 19, 'suck': 821, 'goe': 348, 'someon': 783, 'sit': 773, 'hahaha': 384, 'omg': 620, 'hr': 423, 'play': 657, 'dead': 207, 'friend': 316, 'help': 406, 'almost': 18, 'boy': 102, 'drink': 227, 'wanna': 925, 'open': 625, 'broken': 109, 'weird': 947, 'app': 37, 'pretti': 672, 'cool': 173, 'great': 369, 'great time': 371, 'ladi': 476, 'tweet': 893, 'impress': 440, 'eat': 237, 'sinc': 769, 'last': 479, 'thought': 857, 'slept': 777, 'glad': 336, 'last night': 481, 'sure': 831, 'em': 240, 'cri': 186, 'oh well': 615, 'town': 879, 'two': 896, 'dinner': 215, 'dark': 198, 'chocol': 147, 'twitter': 895, 'number': 611, 'bit': 85, 'voic': 917, 'heat': 402, 'noth': 609, 'sigh': 767, 'alway': 26, 'disappoint': 217, 'everyon': 254, 'might': 557, 'els': 239, 'bring': 106, 'tho': 855, 'late': 482, 'man': 539, 'connect': 170, 'game': 323, 'bag': 61, 'gotta': 367, 'ill': 437, 'call': 121, 'live': 508, 'dont': 222, 'till': 864, 'gone': 350, 'mind': 562, 'watch': 936, 'ate': 47, 'cours': 179, 'record': 707, 'decid': 209, 'die': 213, 'friday': 314, 'sad': 726, 'e': 233, 'friday night': 315, 'sunshin': 827, 'better': 79, 'part': 637, 'shut': 763, 'comput': 166, 'readi': 698, 'get readi': 329, 'exam': 257, 'tom': 871, 'top': 875, 'kinda': 470, 'actual': 3, 'leav': 490, 'colleg': 159, 'ye': 987, 'tri': 885, 'gonna': 351, 'ticket': 862, 'vega': 910, 'wait till': 922, 'summer': 823, 'anyon': 33, 'googl': 362, 'look like': 520, 'unfortun': 901, 'abl': 0, 'nd': 592, 'gig': 333, 'everyth': 255, 'tv': 892, 'ah': 12, 'idea': 434, 'awesom': 51, 'hair': 385, 'cut': 190, 'homework': 415, 'fri': 313, 'chicken': 143, 'everi': 252, 'song': 788, 'new': 597, 'soo': 789, 'long': 514, 'poor': 663, 'crazi': 184, 'big': 82, 'babi': 56, 'kind': 469, 'puppi': 684, 'knew': 472, 'lil': 502, 'set': 751, 'quot': 688, 'nobodi': 605, 'lunch': 528, 'alreadi': 22, 'ur': 905, 'repli': 713, 'problem': 675, 'sorri': 794, 'forward': 308, 'share': 754, 'look forward': 518, 'okay': 618, 'invit': 446, 'mine': 563, 'rock': 722, 'ahh': 13, 'parti': 638, 'seem': 743, 'jealou': 453, 'john': 455, 'definit': 210, 'yea': 988, 'r': 690, 'kid': 467, 'feel like': 283, 'best': 76, 'xd': 981, 'best friend': 77, 'bed': 72, 'beauti': 70, 'go bed': 340, 'tomorrow': 872, 'sunday': 825, 'go away': 338, 'enjoy': 244, 'parent': 635, 'revis': 717, 'littl': 507, 'apart': 36, 'id': 433, 'use': 907, 'whole': 954, 'hun': 428, 'french': 312, 'earli': 234, 'tell': 843, 'without': 960, 'copi': 174, 'yesterday': 993, 'although': 25, 'read': 697, 'yet': 994, 'yr': 997, 'told': 870, 'clean': 151, 'present': 671, 'ever': 251, 'wow': 976, 'realli good': 702, 'book': 96, 'n': 589, 'stuff': 819, 'buy': 117, 'nap': 591, 'phone': 647, 'convers': 171, 'cat': 132, 'walk': 924, 'block': 90, 'name': 590, 'bloodi': 92, 'give': 335, 'boyfriend': 103, 'quot quot': 689, 'us': 906, 'air': 16, 'red': 709, 'happen': 389, 'lucki': 527, 'xxx': 984, 'wanna go': 926, 'go see': 344, 'put': 685, 'forgot': 307, 'min': 561, 'b': 54, 'get better': 328, 'singl': 771, 'amaz': 27, 'ask': 44, 'th': 847, 'holiday': 413, 'rest': 714, 'rain': 693, 'hi': 408, 'quit': 687, 'weather': 941, 'nite': 604, 'dude': 231, 'sorri hear': 795, 'chillin': 145, 'mean': 547, 'bu': 112, 'train': 882, 'catch': 133, 'gut': 378, 'came': 122, 'last day': 480, 'day today': 206, 'hurt': 430, 'nice': 602, 'iphon': 447, 'bike': 83, 'visit': 916, 'extra': 264, 'traffic': 881, 'welcom': 948, 'would': 974, 'save': 732, 'saturday': 731, 'blast': 88, 'must': 587, 'xx': 983, 'sore': 793, 'ya': 985, 'goodnight': 361, 'lot': 523, 'wont': 963, 'stomach': 812, 'pain': 633, 'fail': 270, 'download': 224, 'answer': 31, 'fan': 274, 'question': 686, 'hospit': 419, 'teach': 842, 'especi': 247, 'swine': 835, 'flu': 302, 'pray': 670, 'swine flu': 836, 'travel': 883, 'ignor': 436, 'meant': 548, 'direct': 216, 'cold': 158, 'fli': 300, 'headach': 398, 'pass': 639, 'place': 655, 'wtf': 979, 'lesson': 494, 'sing': 770, 'gt': 375, 'group': 373, 'either': 238, 'mention': 552, 'believ': 75, 'kill': 468, 'yeah': 989, 'near': 593, 'wrong': 978, 'hit': 410, 'car': 128, 'music': 586, 'feet': 285, 'cake': 120, 'momma': 570, 'crap': 182, 'cover': 181, 'tour': 878, 'heard': 400, 'memori': 550, 'card': 129, 'realiz': 700, 'c': 119, 'race': 691, 'sometim': 785, 'life': 498, 'absolut': 1, 'congrat': 169, 'agre': 11, 'son': 787, 'worst': 972, 'depress': 211, 'p': 629, 'go back': 339, 'ago': 10, 'long time': 515, 'mum': 585, 'album': 17, 'yay': 986, 'find': 292, 'thing': 853, 'white': 953, 'remind': 712, 'luv': 529, 'front': 317, 'behind': 74, 'box': 101, 'like go': 501, 'good morn': 357, 'horribl': 418, 'free': 311, 'return': 715, 'bug': 113, 'product': 676, 'pack': 630, 'cloth': 153, 'watch movi': 937, 'lay': 486, 'pool': 662, 'happi birthday': 391, 'photo': 648, 'money': 573, 'tip': 866, 'etc': 248, 'church': 148, 'public': 682, 'dear': 208, 'perfect': 644, 'heart': 401, 'citi': 149, 'camera': 123, 'also': 24, 'store': 814, 'vote': 918, 'send': 747, 'food': 304, 'june': 462, 'see u': 742, 'hell': 404, 'grow': 374, 'view': 915, 'site': 774, 'real': 699, 'blog': 91, 'list': 505, 'hehe': 403, 'ice': 431, 'cream': 185, 'ice cream': 432, 'pick': 650, 'caus': 134, 'want go': 928, 'celebr': 136, 'job': 454, 'sort': 796, 'onlin': 623, 'figur': 289, 'tri get': 886, 'ugh': 898, 'possibl': 665, 'wash': 933, 'short': 759, 'wait see': 921, 'tea': 841, 'differ': 214, 'chang': 138, 'link': 504, 'fell': 286, 'coupl': 178, 'shall': 752, 'notic': 610, 'class': 150, 'work today': 967, 'gift': 332, 'dad': 194, 'cousin': 180, 'shot': 760, 'thank much': 851, 'mad': 531, 'hungri': 429, 'shame': 753, 'face': 267, 'full': 319, 'aww': 52, 'btw': 111, 'miss u': 566, 'bank': 64, 'monday': 572, 'lazi': 487, 'bank holiday': 65, 'broke': 108, 'fix': 299, 'need get': 595, 'afternoon': 8, 'terribl': 844, 'point': 661, 'sunni': 826, 'recov': 708, 'window': 957, 'half': 386, 'damn': 195, 'pull': 683, 'sat': 730, 'outsid': 628, 'care': 130, 'page': 631, 'rip': 720, 'mama': 538, 'profil': 677, 'feel better': 281, 'better soon': 80, 'order': 626, 'arriv': 43, 'concert': 167, 'sign': 768, 'god': 347, 'studi': 818, 'sleepi': 776, 'screw': 738, 'head': 397, 'day mother': 205, 'famili': 273, 'swim': 834, 'plan': 656, 'annoy': 29, 'ju': 459, 'ppl': 668, 'chines': 146, 'drunk': 230, 'due': 232, 'funni': 321, 'count': 177, 'sent': 748, 'feel bad': 280, 'beat': 69, 'doesnt': 219, 'huge': 426, 'test': 845, 'stuck': 817, 'second': 740, 'feel good': 282, 'park': 636, 'whatev': 952, 'wors': 971, 'math': 542, 'ipod': 448, 'yum': 998, 'seen': 745, 'code': 156, 'plu': 659, 'far': 276, 'turn': 891, 'peac': 642, 'si': 764, 'facebook': 268, 'dog': 220, 'shower': 762, 'laugh': 484, 'hmm': 411, 'anyway': 35, 'hard': 394, 'beach': 68, 'cancel': 124, 'mani': 541, 'internet': 444, 'jump': 461, 'everybodi': 253, 'write': 977, 'interview': 445, 'blue': 93, 'expens': 262, 'past': 640, 'moment': 569, 'vid': 913, 'good night': 358, 'side': 766, 'gave': 325, 'brother': 110, 'relax': 710, 'age': 9, 'star': 804, 'trek': 884, 'star trek': 805, 'first time': 297, 'men': 551, 'perform': 645, 'snl': 781, 'ooh': 624, 'becom': 71, 'good thing': 359, 'would like': 975, 'prom': 679, 'high': 409, 'middl': 556, 'danc': 196, 'worth': 973, 'close': 152, 'miley': 559, 'bday': 67, 'lone': 513, 'soooo': 792, 'serious': 749, 'bodi': 94, 'huh': 427, 'wed': 944, 'trip': 887, 'offici': 613, 'that': 852, 'case': 131, 'sound': 797, 'go sleep': 345, 'episod': 246, 'join': 456, 'ha': 381, 'around': 42, 'st': 802, 'beer': 73, 'come back': 161, 'stick': 810, 'war': 930, 'happi star': 393, 'star war': 806, 'war day': 931, 'may th': 545, 'ouch': 627, 'along': 20, 'boo': 95, 'doctor': 218, 'pm': 660, 'power': 667, 'load': 510, 'ass': 46, 'usual': 908, 'mr': 581, 'cheer': 141, 'track': 880, 'enough': 245, 'fantast': 275, 'email': 241, 'back home': 58, 'insid': 441, 'room': 723, 'woo': 964, 'hot': 420, 'minut': 564, 'fine': 293, 'bye': 118, 'lmao': 509, 'hold': 412, 'light': 499, 'piec': 652, 'paper': 634, 'road': 721, 'f': 266, 'dress': 226, 'reason': 706, 'youtub': 996, 'mommi': 571, 'want see': 929, 'instead': 442, 'upload': 903, 'crash': 183, 'didnt': 212, 'much fun': 584, 'offic': 612, 'yummi': 999, 'kiss': 471, 'make sad': 536, 'dang': 197, 'line': 503, 'ride': 718, 'manag': 540, 'anymor': 32, 'earlier': 235, 'g': 322, 'suggest': 822, 'ps': 681, 'mayb': 546, 'stori': 815, 'version': 911, 'forev': 305, 'bowl': 100, 'pizza': 654, 'breakfast': 105, 'servic': 750, 'rd': 696, 'longer': 516, 'sadli': 727, 'gym': 380, 'back work': 59, 'one day': 622, 'anoth': 30, 'support': 829, 'less': 493, 'experi': 263, 'season': 739, 'cannot': 125, 'addict': 6, 'bought': 98, 'worri': 970, 'flight': 301, 'sound like': 798, 'could go': 176, 'year old': 991, 'troubl': 888, 'law': 485, 'spend': 800, 'da': 193, 'alot': 21, 'word': 965, 'realli want': 705, 'compani': 164, 'break': 104, 'cook': 172, 'wife': 955, 'babe': 55, 'smile': 780, 'jay': 452, 'project': 678, 'fact': 269, 'comment': 163, 'felt': 287, 'event': 250, 'juli': 460, 'yep': 992, 'im go': 439, 'go get': 341, 'hubbi': 424, 'come home': 162, 'win': 956, 'websit': 943, 'idk': 435, 'myspac': 588, 'mac': 530, 'mess': 553, 'lie': 497, 'issu': 450, 'aw': 48, 'bout': 99, 'hope get': 417, 'easi': 236, 'except': 258, 'bless': 89, 'bro': 107, 'proud': 680, 'spent': 801, 'appar': 38, 'work tomorrow': 968, 'hello': 405, 'water': 938, 'chat': 139, 'bitch': 86, 'met': 555, 'small': 779, 'london': 512, 'thursday': 860, 'got back': 365, 'v': 909, 'what': 951, 'ball': 62, 'tummi': 890, 'leg': 492, 'taken': 838, 'fight': 288, 'bum': 114, 'mail': 533, 'day everyon': 202, 'piss': 653, 'paid': 632, 'band': 63, 'hahah': 383, 'cd': 135, 'english': 243, 'fair': 271, 'complet': 165, 'practic': 669, 'cup': 188, 'burn': 115, 'look good': 519, 'understand': 900, 'pop': 664, 'bgt': 81, 'great day': 370, 'uk': 899, 'afraid': 7, 'joke': 457, 'account': 2, 'throat': 859, 'daughter': 200, 'somewher': 786, 'goin': 349, 'thank god': 850, 'appreci': 40, 'gonna miss': 353, 'isnt': 449, 'ruin': 724, 'drop': 229, 'via': 912, 'bbq': 66, 'confus': 168, 'rather': 695, 'normal': 607, 'gorgeou': 363, 'touch': 877, 'gettin': 331, 'favorit': 278, 'get see': 330, 'ive': 451, 'mobil': 567, 'make sure': 537, 'feel sick': 284, 'chanc': 137, 'xoxo': 982, 'surpris': 832, 'radio': 692, 'wast': 934, 'shoe': 757, 'chees': 142, 'seem like': 744, 'mile': 558, 'gonna go': 352, 'l': 474, 'lame': 477, 'review': 716, 'arm': 41, 'milk': 560, 'yo': 995, 'day love': 203, 'much better': 583, 'thx': 861, 'slow': 778, 'current': 189, 'web': 942, 'club': 154, 'state': 808, 'learn': 488, 'thank follow': 849, 'exhaust': 260, 'togeth': 869, 'matter': 543, 'alright': 23, 'appl': 39, 'havent': 396, 'sold': 782, 'realli realli': 704, 'bet': 78, 'kick': 466, 'chill': 144, 'expect': 261, 'go miss': 343, 'wat': 935, 'ohh': 616, 'aint': 15, 'make feel': 535}\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer also creates a vocabulary. Checking Vocabulary\n",
    "print('TF IDF VECTORIZER')\n",
    "print('VOCABULARY SIZE: ', len(tfidf_vect_ngram.vocabulary_))\n",
    "print('VOCABULARY CONTENT: ', tfidf_vect_ngram.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train_bow = count_vector.transform(X_train).toarray()\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_bow = count_vector.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize BoW features in training and test set\n",
    "X_train_bow = pr.normalize(X_train_bow, axis=1)\n",
    "X_test_bow  = pr.normalize(X_test_bow, axis=1)\n",
    "\n",
    "X_train_tfidf_ngram = pr.normalize(X_train_tfidf_ngram, axis=1)\n",
    "X_test_tfidf_ngram  = pr.normalize(X_test_tfidf_ngram, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "\n",
    "- Gradient Boosting\n",
    "- Random Forest\n",
    "- K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT BOOSTING FOR COUNT VECTORIZER\n",
      "0.6675455116133082\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT BOOSTING FOR COUNT VECTORIZER\n",
    "xgboost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train_bow, y_train)\n",
    "\n",
    "print('GRADIENT BOOSTING FOR COUNT VECTORIZER')\n",
    "print(xgboost.score(X_test_bow, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT BOOSTING FOR TF-IDF\n",
      "0.6843691148775894\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT BOOSTING FOR TF-IDF\n",
    "xgboost_tf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=13).fit(X_train_tfidf_ngram, y_train)\n",
    "\n",
    "print('GRADIENT BOOSTING FOR TF-IDF')\n",
    "print(xgboost_tf.score(X_test_tfidf_ngram, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST FOR COUNT VECTORIZER\n",
      "0.6288763339610797\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST FOR COUNT VECTORIZER\n",
    "Ran_For = RandomForestClassifier(max_depth=2, n_estimators=100, criterion='entropy')\n",
    "Ran_For.fit(X_train_bow, y_train)\n",
    "\n",
    "print('RANDOM FOREST FOR COUNT VECTORIZER')\n",
    "print(Ran_For.score(X_test_bow, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST FOR TF-IDF\n",
      "0.6018832391713748\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST FOR TF-IDF\n",
    "Ran_For_tf = RandomForestClassifier(max_depth=2, n_estimators=100, criterion='entropy')\n",
    "Ran_For_tf.fit(X_train_tfidf_ngram, y_train)\n",
    "\n",
    "print('RANDOM FOREST FOR TF-IDF')\n",
    "print(Ran_For_tf.score(X_test_tfidf_ngram, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN FOR COUNT VECTORIZER\n",
      "0.5249215317011927\n"
     ]
    }
   ],
   "source": [
    "# KNN FOR COUNT VECTORIZER\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train_bow, y_train)\n",
    "\n",
    "print('KNN FOR COUNT VECTORIZER')\n",
    "print(neigh.score(X_test_bow, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test    - [0] \n",
      "\n",
      "XGB  CV - [1]\n",
      "RaFo CV - [1]\n",
      "K NN CV - [0]\n",
      "XGB  TF - [1]\n",
      "RaFo TF - [1]\n",
      "K NN TF - [0]\n"
     ]
    }
   ],
   "source": [
    "sel = 6\n",
    "print('Test    -',y_test[sel:sel+1],'\\n')\n",
    "\n",
    "print('XGB  CV -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('RaFo CV -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('K NN CV -',neigh.predict(X_test_bow[sel:sel+1]))\n",
    "\n",
    "print('XGB  TF -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('RaFo TF -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('K NN TF -',neigh.predict(X_test_bow[sel:sel+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5469, 1: 5334, 2: 5421}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5471167369901547"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = './saved_models/XGBoost_tf_Generic.sav'\n",
    "pickle.dump(xgboost_tf, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
