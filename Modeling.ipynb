{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DEPENDENCIES\n",
    "\n",
    "## <span style=\"color:red\"> MUST RUN ALWAYS</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliorenteria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Checkpoints\n",
    "import pickle\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Preprocessing\n",
    "import re    # RegEx for removing non-letter characters\n",
    "import nltk  #natural language processing\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.preprocessing as pr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'airline_tweets.csv'\n",
    "#file_name = 'covid_tweets.csv'\n",
    "#file_name = 'generic_tweets.csv'\n",
    "\n",
    "data = pd.read_csv(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asinging a random number to each observation for future sampling\n",
    "np.random.seed(11)\n",
    "data['rand'] = np.random.random_sample(size=(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.724934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.420204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment      rand\n",
       "0                @VirginAmerica What @dhepburn said.          0  0.180270\n",
       "1  @VirginAmerica plus you've added commercials t...          1  0.019475\n",
       "2  @VirginAmerica I didn't today... Must mean I n...          0  0.463219\n",
       "3  @VirginAmerica it's really aggressive to blast...         -1  0.724934\n",
       "4  @VirginAmerica and it's a really big bad thing...         -1  0.420204"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE - Total:  9178 , Proportion:  0.2574634996731314\n",
      "POSITIVE - Total:  3099 , Proportion:  0.7625040335592127\n",
      "POSITIVE - Total:  2363 , Proportion:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Sampling the min ammount from the other to to obtain a balanced dataset\n",
    "\n",
    "samp_neg = min(data.sentiment.value_counts())/data.sentiment.value_counts()[-1]\n",
    "samp_neu = min(data.sentiment.value_counts())/data.sentiment.value_counts()[0]\n",
    "samp_pos = min(data.sentiment.value_counts())/data.sentiment.value_counts()[1]\n",
    "\n",
    "print('POSITIVE - Total: ',data.sentiment.value_counts()[-1], ', Proportion: ',samp_neg)\n",
    "print('POSITIVE - Total: ',data.sentiment.value_counts()[0], ', Proportion: ',samp_neu)\n",
    "print('POSITIVE - Total: ',data.sentiment.value_counts()[1], ', Proportion: ',samp_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    2397\n",
      " 1    2363\n",
      " 0    2350\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica  I flew from NYC to SFO last we...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.116737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica you guys messed up my seating.....</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.111661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica status match program.  I applie...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.083953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.055674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica hi! i'm so excited about your $...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.062888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment      rand\n",
       "0  @VirginAmerica  I flew from NYC to SFO last we...         -1  0.116737\n",
       "1  @VirginAmerica you guys messed up my seating.....         -1  0.111661\n",
       "2  @VirginAmerica status match program.  I applie...         -1  0.083953\n",
       "3  @VirginAmerica amazing to me that we can't get...         -1  0.055674\n",
       "4  @VirginAmerica hi! i'm so excited about your $...         -1  0.062888"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the sampled datasets for Negative, Neutral and Positive\n",
    "frames = [data.loc[(data['sentiment'] == -1) & (data['rand'] <= samp_neg)],\n",
    "data.loc[(data['sentiment'] == 0) & (data['rand'] <= samp_neu)],\n",
    "data.loc[(data['sentiment'] == 1) & (data['rand'] <= samp_pos)]]\n",
    "\n",
    "data = pd.concat(frames)\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.sentiment.value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZING\n",
    "\n",
    "Separate tweets into lists of word (or components) that carry meanings.\n",
    "\n",
    "Removing/converting the following elements:\n",
    "- Uppercase\n",
    "- Urls\n",
    "- Simbols\n",
    "- Numbers\n",
    "- English Stopwords\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format, tokenize and remove stopwords from tweets.\n",
    "def tweet_to_words(tweet):\n",
    "    ''' Convert tweet text into a sequence of words '''\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = tweet.lower()\n",
    "    # remove tweeter users and hashtags ( @xxx, #xxx )\n",
    "    text = re.sub(r\"[@#]\\w+\", \" \", text)\n",
    "    # remove https\n",
    "    text = re.sub(r\"[(http(s)?):\\/\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)\", \" \", text)\n",
    "    #text = \" \".join(filter(lambda w: w.find(\"https://\") == -1, text.split(\" \")))\n",
    "    # remove non letters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    #remove numbers\n",
    "    text = re.sub(r\"[0-9]*\", \"\", text)\n",
    "    # tokenize\n",
    "    words = text.split()\n",
    "    # remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    # apply stemming\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    # return list\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL TWEET ->\n",
      " @VirginAmerica and it's a really big bad thing about it\n",
      "\n",
      "PROCESSED TWEET ->\n",
      " ['realli', 'big', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# Testing tweet_to_words Function\n",
    "sel = 4\n",
    "print(\"\\nORIGINAL TWEET ->\\n\", data['text'][sel])\n",
    "print(\"\\nPROCESSED TWEET ->\\n\", tweet_to_words(data['text'][sel]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055253dc403749faa62491e9d6ecf9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14640.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to each tweet and store as X\n",
    "X = []\n",
    "for i in trange(len(data['text'])):\n",
    "    X.append(tweet_to_words(data['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(data['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to each tweet and store as X\n",
    "# X = list(map(tweet_to_words, data['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš© CHECKPOINT \n",
    "\n",
    "Saving **Tokenized List** as a Pickle File to retrieve latter and save memory and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving X to a PICKLE to retrieve back latter:\n",
    "\n",
    "pickle_dic = {'df_1245.csv':'1245_',\n",
    "              'airline_tweets.csv':'airlines_',\n",
    "              'covid_tweets.csv':'covid_',\n",
    "              'generic_tweets.csv':'generic_'}\n",
    "\n",
    "# Code to Save PICKLE\n",
    "#with open(pickle_dic[file_name]+'X.pkl', 'wb') as f:\n",
    "#    pickle.dump(X, f)\n",
    "#with open(pickle_dic[file_name]+'Y.pkl', 'wb') as f:\n",
    "#    pickle.dump(Y, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Retrieve PICKLE\n",
    "with open(pickle_dic[file_name]+'X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open(pickle_dic[file_name]+'Y.pkl', 'rb') as f:\n",
    "    Y = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in the total set :    14640\n",
      "Number of tweets in the training set : 10248\n",
      "Number of tweets in the testing set :  4392\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=13)\n",
    "\n",
    "print('Number of tweets in the total set :    {}'.format(len(X)))\n",
    "print('Number of tweets in the training set : {}'.format(len(X_train)))\n",
    "print('Number of tweets in the testing set :  {}'.format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6411, 1: 2172, 2: 1665}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
       "                ngram_range=(1, 1),\n",
       "                preprocessor=<function <lambda> at 0x7f86f6c680d0>,\n",
       "                stop_words=None, strip_accents=None,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function <lambda> at 0x7f86f6c68158>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "vocabulary_size = 100    # Rounded up from 17422\n",
    "\n",
    "# Generating Bag of Words\n",
    "# Tweets have already been preprocessed hence dummy function will be passed in to preprocessor & tokenizer step\n",
    "count_vector = CountVectorizer(max_features=vocabulary_size, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "count_vector.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY SIZE:  100\n",
      "VOCABULARY CONTENT:  {'pleas': 65, 'connect': 14, 'plane': 64, 'book': 9, 'problem': 66, 'travel': 85, 'due': 21, 'passeng': 61, 'make': 51, 'check': 13, 'get': 30, 'back': 6, 'delay': 19, 'flight': 27, 'would': 99, 'like': 47, 'email': 22, 'lost': 49, 'today': 82, 'fli': 26, 'even': 23, 'gate': 29, 'us': 90, 'tri': 86, 'guy': 35, 'time': 81, 'want': 94, 'custom': 17, 'servic': 74, 'could': 15, 'bag': 7, 'amp': 4, 'use': 91, 'ticket': 80, 'miss': 54, 'cancel': 11, 'day': 18, 'got': 33, 'thank': 79, 'unit': 89, 'agent': 1, 'help': 36, 'told': 83, 'go': 31, 'never': 56, 'wait': 93, 'know': 43, 'see': 73, 'home': 38, 'aa': 0, 'seat': 72, 'call': 10, 'way': 95, 'chang': 12, 'phone': 63, 'worst': 98, 'sit': 75, 'love': 50, 'board': 8, 'good': 32, 'respons': 70, 'realli': 67, 'take': 78, 'rebook': 68, 'tomorrow': 84, 'airport': 3, 'hour': 40, 'new': 57, 'airlin': 2, 'flightl': 28, 'still': 77, 'number': 59, 'reserv': 69, 'one': 60, 'trip': 87, 'issu': 42, 'crew': 16, 'hr': 41, 'min': 52, 'hold': 37, 'hope': 39, 'someon': 76, 'look': 48, 'minut': 53, 'first': 25, 'u': 88, 'anoth': 5, 'weather': 96, 'late': 45, 'great': 34, 'work': 97, 'say': 71, 'ever': 24, 'need': 55, 'w': 92, 'peopl': 62, 'next': 58, 'let': 46, 'dm': 20, 'last': 44}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer creates a vocabulary. Checking Vocabulary\n",
    "print('VOCABULARY SIZE: ', len(count_vector.vocabulary_))\n",
    "print('VOCABULARY CONTENT: ', count_vector.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train_bow = count_vector.transform(X_train).toarray()\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_bow = count_vector.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments from here on out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=2, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ran_For = RandomForestClassifier(max_depth=2, n_estimators=100, criterion='entropy')\n",
    "Ran_For.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ran_For.predict(X_test_bow[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 43031, 1: 53394, 2: 71051}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.score(X_test_bow, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.predict(X_test_bow[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
