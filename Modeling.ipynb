{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DEPENDENCIES\n",
    "\n",
    "## <span style=\"color:red\"> MUST RUN ALWAYS</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juliorenteria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Checkpoints\n",
    "import pickle\n",
    "import cloudpickle\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Preprocessing\n",
    "import re    # RegEx for removing non-letter characters\n",
    "import nltk  #natural language processing\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "import sklearn.preprocessing as pr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = './data/airline_tweets.csv'\n",
    "file_name = './data/covid_tweets.csv'\n",
    "#file_name = './data/generic_tweets.csv'\n",
    "\n",
    "data = pd.read_csv(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asinging a random number to each observation for future sampling\n",
    "np.random.seed(11)\n",
    "data['rand'] = np.random.random_sample(size=(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.463219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.420204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment      rand\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...          0  0.180270\n",
       "1  advice Talk to your neighbours family to excha...          1  0.019475\n",
       "2  Coronavirus Australia: Woolworths to give elde...          1  0.463219\n",
       "3  My food stock is not the only one which is emp...          1  0.724934\n",
       "4  Me, ready to go at supermarket during the #COV...         -1  0.420204"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVA - Total:  15398 , Proportion:  0.5009092089881803\n",
      "NEUTRALES - Total:  7713 , Proportion:  1.0\n",
      "POSITIVE - Total:  18046 , Proportion:  0.42740773578632385\n"
     ]
    }
   ],
   "source": [
    "# Sampling the min ammount from the other to to obtain a balanced dataset\n",
    "\n",
    "samp_neg = min(data.sentiment.value_counts())/data.sentiment.value_counts()[-1]\n",
    "samp_neu = min(data.sentiment.value_counts())/data.sentiment.value_counts()[0]\n",
    "samp_pos = min(data.sentiment.value_counts())/data.sentiment.value_counts()[1]\n",
    "\n",
    "print('NEGATIVA - Total: ',data.sentiment.value_counts()[-1], ', Proportion: ',samp_neg)\n",
    "print('NEUTRALES - Total: ',data.sentiment.value_counts()[0], ', Proportion: ',samp_neu)\n",
    "print('POSITIVE - Total: ',data.sentiment.value_counts()[1], ', Proportion: ',samp_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    7755\n",
      " 0    7713\n",
      " 1    7710\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.420204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@10DowningStreet @grantshapps what is being do...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.111661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you see malicious price increases in NYC? T...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.055674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@7SealsOfTheEnd Soon with dwindling supplies u...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.479797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There Is of in the Country  The more empty she...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.401676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment      rand\n",
       "0  Me, ready to go at supermarket during the #COV...         -1  0.420204\n",
       "1  @10DowningStreet @grantshapps what is being do...         -1  0.111661\n",
       "2  Do you see malicious price increases in NYC? T...         -1  0.055674\n",
       "3  @7SealsOfTheEnd Soon with dwindling supplies u...         -1  0.479797\n",
       "4  There Is of in the Country  The more empty she...         -1  0.401676"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the sampled datasets for Negative, Neutral and Positive\n",
    "frames = [data.loc[(data['sentiment'] == -1) & (data['rand'] <= samp_neg)],\n",
    "data.loc[(data['sentiment'] == 0) & (data['rand'] <= samp_neu)],\n",
    "data.loc[(data['sentiment'] == 1) & (data['rand'] <= samp_pos)]]\n",
    "\n",
    "data = pd.concat(frames)\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.sentiment.value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZING\n",
    "\n",
    "Separate tweets into lists of word (or components) that carry meanings.\n",
    "\n",
    "Removing/converting the following elements:\n",
    "- Uppercase\n",
    "- Urls\n",
    "- Simbols\n",
    "- Numbers\n",
    "- English Stopwords\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format, tokenize and remove stopwords from tweets.\n",
    "def tweet_to_words(tweet):\n",
    "    ''' Convert tweet text into a sequence of words '''\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text = tweet.lower()\n",
    "    # remove tweeter users and hashtags ( @xxx, #xxx )\n",
    "    text = re.sub(r\"[@#]\\w+\", \" \", text)\n",
    "    # remove https\n",
    "    text = re.sub(r\"[(http(s)?):\\/\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&\\/\\/=]*)\", \" \", text)\n",
    "    #text = \" \".join(filter(lambda w: w.find(\"https://\") == -1, text.split(\" \")))\n",
    "    # remove non letters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    #remove numbers\n",
    "    text = re.sub(r\"[0-9]*\", \"\", text)\n",
    "    # tokenize\n",
    "    words = text.split()\n",
    "    # remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    # apply stemming\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    # return list\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL TWEET ->\n",
      " One thing about it. Nielsen gonna get their consumer data https://t.co/kGX6wPbZdq\r",
      "\r\n",
      "\r",
      "\r\n",
      "Might dive into this if I have time to help with brainstorming any opportunities here. Because marketing language will shift into these funnels. \r",
      "\r\n",
      "\r",
      "\r\n",
      "Want that? Text 'yes' to 903-689-1975\n",
      "\n",
      "PROCESSED TWEET -> ['one', 'thing', 'nielsen', 'gonna', 'get', 'consum', 'data', 'might', 'dive', 'time', 'help', 'brainstorm', 'opportun', 'market', 'languag', 'shift', 'funnel', 'want', 'text', 'ye']\n",
      "\\SENTIMENT ->  1\n"
     ]
    }
   ],
   "source": [
    "# Testing tweet_to_words Function\n",
    "sel = 16003\n",
    "print(\"\\nORIGINAL TWEET ->\\n\", data['text'][sel])\n",
    "print(\"\\nPROCESSED TWEET ->\", tweet_to_words(data['text'][sel]))\n",
    "print(\"\\SENTIMENT -> \", data['sentiment'][sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793f5fd955a145bb830c667f30c542c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization to each tweet and store as X\n",
    "\n",
    "X = []\n",
    "for i in trange(len(data['text'])):\n",
    "    X.append(tweet_to_words(data['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(data['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(le.inverse_transform([0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to each tweet and store as X\n",
    "# X = list(map(tweet_to_words, data['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš© CHECKPOINT \n",
    "\n",
    "Saving **Tokenized List** as a Pickle File to retrieve latter and save memory and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving X to a PICKLE to retrieve back latter:\n",
    "\n",
    "pickle_dic = {'df_1245.csv':'1245_',\n",
    "              'airline_tweets.csv':'airlines_',\n",
    "              'covid_tweets.csv':'covid_',\n",
    "              'generic_tweets.csv':'generic_'}\n",
    "\n",
    "# Code to Save PICKLE\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'X.pkl', 'wb') as f:\n",
    "#    pickle.dump(X, f)\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'Y.pkl', 'wb') as f:\n",
    "#    pickle.dump(Y, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to Retrieve PICKLE\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'X.pkl', 'rb') as f:\n",
    "#    X = pickle.load(f)\n",
    "#with open('./tokenized_data/'+pickle_dic[file_name]+'Y.pkl', 'rb') as f:\n",
    "#    Y = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in the total set :    23178\n",
      "Number of tweets in the training set : 16224\n",
      "Number of tweets in the testing set :  6954\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=13)\n",
    "\n",
    "print('Number of tweets in the total set :    {}'.format(len(X)))\n",
    "print('Number of tweets in the training set : {}'.format(len(X_train)))\n",
    "print('Number of tweets in the testing set :  {}'.format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DISTRIBUTION -  {0: 5469, 1: 5334, 2: 5421}\n",
      "TESTING DISTRIBUTION -  {0: 2286, 1: 2379, 2: 2289}\n"
     ]
    }
   ],
   "source": [
    "# Counting the distribution in training and test datasets\n",
    "unique_tr, counts_tr = np.unique(y_train, return_counts=True)\n",
    "print('TRAINING DISTRIBUTION - ',dict(zip(unique_tr, counts_tr)))\n",
    "\n",
    "unique_te, counts_te = np.unique(y_test, return_counts=True)\n",
    "print('TESTING DISTRIBUTION - ',dict(zip(unique_te, counts_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "                ngram_range=(1, 1),\n",
       "                preprocessor=<function <lambda> at 0x7f8cb91f47b8>,\n",
       "                stop_words=None, strip_accents=None,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function <lambda> at 0x7f8cb91f4488>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "vocabulary_size = 1000    # Rounded up from 17422\n",
    "\n",
    "# Generating Bag of Words\n",
    "# Tweets have already been preprocessed hence dummy function will be passed in to preprocessor & tokenizer step\n",
    "count_vector = CountVectorizer(max_features=vocabulary_size, preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "count_vector.fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT VECTORIZER\n",
      "VOCABULARY SIZE:  1000\n",
      "VOCABULARY CONTENT:  {'food': 344, 'suppli': 858, 'consum': 179, 'depart': 227, 'govern': 384, 'take': 869, 'effect': 271, 'step': 837, 'prevent': 666, 'black': 96, 'market': 528, 'hoard': 420, 'protect': 679, 'mask': 529, 'hand': 398, 'sanit': 752, 'state': 834, 'group': 389, 'want': 956, 'insur': 453, 'compani': 170, 'help': 410, 'restaur': 730, 'industri': 444, 'taken': 870, 'massiv': 531, 'hit': 419, 'covid': 198, 'crisi': 203, 'pandem': 610, 'bank': 76, 'day': 216, 'payment': 623, 'period': 626, 'need': 568, 'due': 261, 'list': 496, 'credit': 202, 'would': 991, 'thought': 886, 'look': 509, 'like': 492, 'groceri': 388, 'store': 843, 'rather': 694, 'go': 374, 'purchas': 683, 'thing': 883, 'wonder': 982, 'today': 894, 'watch': 962, 'supermarket': 857, 'fight': 331, 'old': 590, 'ladi': 472, 'last': 475, 'roll': 743, 'toilet': 896, 'paper': 613, 'decis': 222, 'let': 488, 'peopl': 624, 'self': 768, 'isol': 457, 'get': 368, 'essenti': 294, 'stop': 842, 'panic': 611, 'buy': 111, 'vulner': 949, 'think': 884, 'ye': 995, 'stock': 840, 'forget': 346, 'run': 746, 'low': 515, 'via': 942, 'shop': 790, 'hour': 426, 'creat': 201, 'elderli': 275, 'custom': 209, 'also': 34, 'includ': 439, 'nh': 573, 'amp': 44, 'support': 859, 'worker': 985, 'amaz': 36, 'cheap': 141, 'deal': 217, 'dm': 246, 'us': 934, 'stay': 836, 'home': 423, 'activ': 8, 'feed': 328, 'chicken': 144, 'make': 520, 'follow': 343, 'email': 278, 'start': 833, 'pm': 646, 'bottl': 99, 'got': 381, 'chain': 135, 'sure': 861, 'older': 591, 'risk': 741, 'american': 39, 'avoid': 67, 'crowd': 205, 'special': 824, 'invest': 456, 'person': 627, 'end': 284, 'quickli': 690, 'say': 757, 'deserv': 228, 'time': 892, 'nurs': 582, 'hospit': 425, 'staff': 830, 'among': 42, 'mani': 524, 'other': 601, 'live': 500, 'announc': 45, 'shut': 795, 'sentiment': 775, 'post': 653, 'biggest': 92, 'drop': 259, 'record': 706, 'earli': 264, 'april': 55, 'free': 350, 'fall': 319, 'confid': 175, 'wors': 988, 'expect': 305, 'infect': 445, 'death': 219, 'rate': 693, 'soon': 819, 'allow': 30, 'economi': 270, 'import': 438, 'messag': 544, 'one': 592, 'anoth': 46, 'good': 380, 'away': 69, 'survey': 865, 'eat': 268, 'fruit': 356, 'veget': 940, 'consid': 178, 'place': 636, 'someth': 818, 'coronaviru': 189, 'came': 116, 'light': 491, 'new': 570, 'recommend': 705, 'wear': 965, 'cloth': 160, 'face': 315, 'cover': 197, 'slow': 809, 'spread': 828, 'public': 682, 'set': 780, 'social': 813, 'distanc': 244, 'measur': 537, 'difficult': 236, 'maintain': 518, 'e': 263, 'pharmaci': 629, 'bring': 105, 'back': 72, 'ration': 695, 'year': 996, 'pretti': 665, 'nice': 574, 'item': 460, 'packag': 607, 'cut': 210, 'crude': 206, 'price': 667, 'demand': 226, 'small': 810, 'busi': 110, 'retail': 734, 'ever': 298, 'famili': 320, 'servic': 779, 'visit': 947, 'local': 502, 'view': 944, 'healthcar': 405, 'may': 533, 'die': 234, 'alon': 32, 'pleas': 641, 'hero': 411, 'stockpil': 841, 'hike': 417, 'understand': 928, 'given': 371, 'temporari': 876, 'know': 470, 'fix': 341, 'onlin': 594, 'woman': 981, 'surviv': 866, 'lower': 516, 'ga': 363, 'issu': 458, 'account': 4, 'health': 404, 'care': 124, 'cost': 191, 'worri': 987, 'track': 907, 'releas': 715, 'week': 968, 'said': 750, 'abl': 0, 'afford': 18, 'treatment': 913, 'rememb': 718, 'keep': 464, 'shame': 782, 'guy': 395, 'school': 761, 'meal': 535, 'guidanc': 394, 'plan': 637, 'report': 721, 'first': 340, 'launch': 479, 'march': 526, 'reveal': 736, 'immedi': 436, 'global': 372, 'impact': 437, 'posit': 651, 'action': 7, 'seen': 767, 'across': 5, 'key': 465, 'sector': 763, 'base': 78, 'u': 924, 'experienc': 307, 'growth': 391, 'spend': 825, 'huge': 430, 'come': 164, 'fill': 333, 'give': 370, 'employe': 281, 'thank': 882, 'corona': 188, 'behavior': 84, 'tomorrow': 898, 'minut': 550, 'answer': 47, 'question': 687, 'refund': 708, 'travel': 911, 'household': 428, 'clear': 154, 'inform': 448, 'best': 89, 'next': 572, 'outbreak': 602, 'work': 984, 'expert': 308, 'empti': 282, 'bread': 103, 'aisl': 27, 'reason': 701, 'still': 838, 'see': 765, 'hear': 407, 'way': 964, 'togeth': 895, 'apart': 51, 'whether': 973, 'outsid': 603, 'dear': 218, 'world': 986, 'shelv': 785, 'sir': 803, 'solut': 816, 'case': 127, 'test': 880, 'kit': 469, 'made': 517, 'avail': 65, 'sale': 751, 'govt': 386, 'lockdown': 505, 'citi': 149, 'done': 251, 'ask': 60, 'friend': 353, 'tri': 915, 'usual': 936, 'water': 963, 'amid': 40, 'member': 543, 'use': 935, 'platform': 639, 'provid': 680, 'commun': 169, 'learn': 483, 'deliveri': 225, 'area': 57, 'love': 514, 'littl': 499, 'card': 123, 'futur': 362, 'info': 447, 'viru': 946, 'situat': 805, 'increas': 441, 'soap': 811, 'oil': 588, 'around': 58, 'continu': 183, 'gallon': 364, 'uncertainti': 927, 'surg': 862, 'firm': 339, 'forc': 345, 'chang': 138, 'habit': 396, 'farmer': 323, 'produc': 673, 'much': 562, 'respect': 726, 'went': 972, 'morn': 559, 'awar': 68, 'reduc': 707, 'cash': 128, 'disinfect': 242, 'necessari': 567, 'kill': 467, 'call': 114, 'clean': 153, 'rs': 744, 'till': 891, 'c': 113, 'appli': 53, 'detail': 232, 'click': 156, 'absolut': 1, 'serious': 777, 'precaut': 660, 'prepar': 662, 'media': 539, 'fear': 325, 'rice': 737, 'student': 850, 'india': 442, 'countri': 195, 'rise': 740, 'number': 581, 'respond': 727, 'closur': 159, 'shock': 789, 'ensur': 288, 'stapl': 832, 'eye': 313, 'open': 595, 'experi': 306, 'possibl': 652, 'w': 950, 'hard': 402, 'dog': 248, 'find': 337, 'put': 685, 'car': 122, 'trolley': 917, 'full': 359, 'left': 486, 'affair': 16, 'agenc': 20, 'alreadi': 33, 'total': 902, 'product': 674, 'feel': 329, 'hold': 422, 'partner': 617, 'resourc': 725, 'anyon': 49, 'great': 387, 'k': 463, 'safe': 748, 'fed': 326, 'queue': 688, 'la': 471, 'buyer': 112, 'tell': 875, 'scare': 760, 'happen': 400, 'though': 885, 'mom': 554, 'true': 919, 'believ': 87, 'could': 193, 'mean': 536, 'life': 490, 'frontlin': 355, 'often': 586, 'accord': 3, 'data': 214, 'driver': 258, 'design': 229, 'webinar': 966, 'relat': 714, 'regist': 711, 'trend': 914, 'research': 723, 'everyon': 300, 'front': 354, 'line': 494, 'appreci': 54, 'forward': 347, 'sinc': 801, 'babi': 71, 'sick': 797, 'cough': 192, 'quarantin': 686, 'order': 599, 'ago': 21, 'slot': 808, 'execut': 304, 'egg': 273, 'without': 980, 'kind': 468, 'stuff': 852, 'drive': 256, 'enter': 289, 'dump': 262, 'milk': 546, 'except': 303, 'gener': 367, 'collaps': 162, 'th': 881, 'drug': 260, 'charg': 139, 'latest': 478, 'everi': 299, 'singl': 802, 'saw': 756, 'pay': 622, 'play': 640, 'rule': 745, 'doctor': 247, 'job': 461, 'employ': 280, 'uk': 925, 'offic': 584, 'goug': 382, 'scam': 758, 'soar': 812, 'news': 571, 'catch': 130, 'sever': 781, 'discuss': 240, 'potenti': 654, 'read': 697, 'meat': 538, 'wast': 961, 'bag': 74, 'ban': 75, 'shopper': 791, 'london': 506, 'note': 578, 'man': 522, 'oper': 596, 'send': 772, 'receiv': 702, 'bill': 93, 'heart': 409, 'goe': 376, 'critic': 204, 'shift': 786, 'relief': 716, 'addit': 12, 'limit': 493, 'park': 615, 'ticket': 890, 'late': 476, 'must': 563, 'manag': 523, 'lot': 513, 'door': 253, 'almost': 31, 'month': 558, 'carri': 125, 'truck': 918, 'pressur': 664, 'ventil': 941, 'realli': 700, 'manufactur': 525, 'war': 957, 'effort': 272, 'chariti': 140, 'serv': 778, 'children': 145, 'class': 152, 'wash': 960, 'better': 90, 'desper': 230, 'phone': 630, 'china': 146, 'disrupt': 243, 'scammer': 759, 'advantag': 14, 'surround': 864, 'feder': 327, 'trade': 908, 'commiss': 166, 'polic': 648, 'slash': 807, 'hygien': 432, 'econom': 269, 'australia': 63, 'per': 625, 'close': 158, 'liter': 498, 'b': 70, 'wage': 951, 'predict': 661, 'becom': 82, 'highlight': 416, 'differ': 235, 'concern': 173, 'push': 684, 'move': 561, 'major': 519, 'page': 608, 'shortag': 793, 'well': 971, 'yet': 998, 'imagin': 435, 'gov': 383, 'street': 846, 'safeti': 749, 'reach': 696, 'connect': 177, 'ppe': 656, 'ship': 787, 'restrict': 732, 'sell': 770, 'deliv': 224, 'less': 487, 'basic': 79, 'explain': 309, 'donat': 250, 'contact': 181, 'power': 655, 'big': 91, 'temporarili': 877, 'regul': 712, 'leav': 485, 'bare': 77, 'certain': 134, 'affect': 17, 'fresh': 351, 'even': 296, 'told': 897, 'recent': 703, 'aid': 25, 'right': 739, 'distribut': 245, 'pasta': 620, 'box': 101, 'societi': 814, 'extra': 311, 'program': 677, 'lead': 481, 'glove': 373, 'target': 872, 'near': 566, 'term': 878, 'meet': 542, 'half': 397, 'sign': 799, 'disabl': 239, 'citizen': 150, 'updat': 932, 'alert': 29, 'top': 901, 'might': 545, 'date': 215, 'speak': 823, 'book': 98, 'check': 142, 'bit': 95, 'fell': 330, 'alway': 35, 'remain': 717, 'chines': 147, 'nation': 565, 'paid': 609, 'simpl': 800, 'normal': 577, 'thursday': 889, 'medic': 540, 'tp': 906, 'sunday': 856, 'farm': 322, 'past': 619, 'veg': 939, 'selfish': 769, 'share': 783, 'common': 168, 'financi': 336, 'link': 495, 'tip': 893, 'challeng': 136, 'part': 616, 'develop': 233, 'cannot': 120, 'vaccin': 937, 'co': 161, 'grow': 390, 'insight': 451, 'act': 6, 'blog': 97, 'worst': 989, 'refus': 709, 'rent': 720, 'owner': 604, 'contract': 184, 'respons': 728, 'access': 2, 'volunt': 948, 'easter': 267, 'readi': 698, 'electr': 276, 'show': 794, 'save': 755, 'miss': 551, 'itali': 459, 'collect': 163, 'presid': 663, 'encourag': 283, 'associ': 62, 'restock': 731, 'unit': 930, 'agricultur': 23, 'websit': 967, 'sanitis': 753, 'amidst': 41, 'complaint': 171, 'daili': 211, 'caus': 131, 'canadian': 118, 'pick': 632, 'least': 484, 'instead': 452, 'yesterday': 997, 'poor': 650, 'someon': 817, 'commerc': 165, 'wife': 976, 'mind': 548, 'els': 277, 'declin': 223, 'found': 348, 'event': 297, 'offici': 585, 'requir': 722, 'pet': 628, 'trip': 916, 'talk': 871, 'contain': 182, 'pile': 635, 'sourc': 820, 'fact': 316, 'driven': 257, 'folk': 342, 'bad': 73, 'crash': 199, 'south': 821, 'minist': 549, 'fund': 361, 'idea': 433, 'danger': 213, 'expos': 310, 'recess': 704, 'plu': 643, 'etc': 295, 'amazon': 37, 'wait': 952, 'long': 507, 'studi': 851, 'exampl': 302, 'stress': 847, 'wish': 979, 'touch': 903, 'rais': 692, 'notic': 580, 'stupid': 853, 'crazi': 200, 'locat': 503, 'condit': 174, 'later': 477, 'offer': 583, 'lost': 512, 'threat': 887, 'town': 905, 'turn': 921, 'money': 557, 'enough': 287, 'incom': 440, 'walk': 954, 'bu': 107, 'saudi': 754, 'white': 974, 'surpris': 863, 'pack': 606, 'control': 185, 'short': 792, 'rest': 729, 'current': 208, 'ongo': 593, 'prioriti': 668, 'side': 798, 'join': 462, 'assist': 61, 'regular': 713, 'ridicul': 738, 'actual': 9, 'healthi': 406, 'hey': 412, 'non': 576, 'final': 334, 'st': 829, 'station': 835, 'mark': 527, 'air': 26, 'guess': 393, 'seem': 766, 'dollar': 249, 'cancel': 119, 'advic': 15, 'fail': 317, 'fake': 318, 'brought': 106, 'everyth': 301, 'file': 332, 'pictur': 633, 'night': 575, 'calm': 115, 'amount': 43, 'york': 999, 'fulli': 360, 'stori': 844, 'loss': 511, 'anyth': 50, 'medicin': 541, 'privat': 669, 'hope': 424, 'initi': 449, 'fine': 338, 'moment': 555, 'confirm': 176, 'remind': 719, 'cart': 126, 'canada': 117, 'inflat': 446, 'break': 104, 'n': 564, 'second': 762, 'worth': 990, 'anxieti': 48, 'properti': 678, 'probabl': 670, 'option': 598, 'practic': 658, 'ppl': 657, 'noth': 579, 'human': 431, 'pub': 681, 'suggest': 855, 'tesco': 879, 'never': 569, 'fast': 324, 'walmart': 955, 'high': 414, 'build': 108, 'cashier': 129, 'sold': 815, 'monday': 556, 'mother': 560, 'space': 822, 'either': 274, 'coupl': 196, 'longer': 508, 'claim': 151, 'point': 647, 'higher': 415, 'emerg': 279, 'checkout': 143, 'p': 605, 'strategi': 845, 'regard': 710, 'interest': 454, 'piec': 634, 'counti': 194, 'pantri': 612, 'address': 13, 'dont': 252, 'fuck': 357, 'despit': 231, 'stimulu': 839, 'law': 480, 'trump': 920, 'trader': 909, 'ad': 10, 'dairi': 212, 'hous': 427, 'tonight': 899, 'profit': 676, 'direct': 238, 'bulk': 109, 'debt': 220, 'clerk': 155, 'diseas': 241, 'especi': 293, 'equip': 292, 'extrem': 312, 'articl': 59, 'problem': 671, 'friday': 352, 'basket': 80, 'shelf': 784, 'epidem': 291, 'mayb': 534, 'secur': 764, 'head': 403, 'polici': 649, 'insid': 450, 'mall': 521, 'write': 992, 'return': 735, 'two': 923, 'patient': 621, 'video': 943, 'model': 553, 'bought': 100, 'larg': 473, 'virtual': 945, 'urg': 933, 'arabia': 56, 'profession': 675, 'kid': 466, 'senior': 773, 'real': 699, 'shutdown': 796, 'earn': 265, 'word': 983, 'behind': 86, 'cure': 207, 'enforc': 286, 'spike': 826, 'took': 900, 'r': 691, 'listen': 497, 'symptom': 867, 'complet': 172, 'ill': 434, 'wrong': 993, 'ceo': 133, 'weekend': 969, 'team': 874, 'bc': 81, 'warehous': 958, 'happi': 401, 'america': 38, 'suffer': 854, 'level': 489, 'financ': 335, 'consumpt': 180, 'tax': 873, 'leader': 482, 'pass': 618, 'struggl': 849, 'parent': 614, 'million': 547, 'capit': 121, 'author': 64, 'plenti': 642, 'matter': 532, 'hire': 418, 'seriou': 776, 'cent': 132, 'wipe': 978, 'site': 804, 'warn': 959, 'commod': 167, 'agre': 22, 'plummet': 644, 'transport': 910, 'entir': 290, 'mass': 530, 'shit': 788, 'app': 52, 'sent': 774, 'result': 733, 'ahead': 24, 'individu': 443, 'far': 321, 'three': 888, 'gold': 377, 'spot': 827, 'process': 672, 'lose': 510, 'decid': 221, 'treat': 912, 'system': 868, 'gonna': 379, 'begin': 83, 'strong': 848, 'seller': 771, 'age': 19, 'stand': 831, 'photo': 631, 'governor': 385, 'cope': 187, 'behaviour': 85, 'pre': 659, 'heard': 408, 'mobil': 552, 'unpreced': 931, 'russia': 747, 'whole': 975, 'largest': 474, 'easi': 266, 'toward': 904, 'energi': 285, 'dr': 255, 'hi': 413, 'unemploy': 929, 'howev': 429, 'chanc': 137, 'fuel': 358, 'gather': 366, 'opportun': 597, 'brand': 102, 'quick': 689, 'four': 349, 'ok': 589, 'benefit': 88, 'add': 11, 'skyrocket': 806, 'lock': 504, 'averag': 66, 'road': 742, 'wine': 977, 'corpor': 190, 'plant': 638, 'unabl': 926, 'f': 314, 'resid': 724, 'handl': 399, 'giant': 369, 'suppos': 860, 'weekli': 970, 'loan': 501, 'alcohol': 28, 'doubl': 254, 'digit': 237, 'gt': 392, 'billion': 94, 'x': 994, 'wake': 953, 'plung': 645, 'valu': 938, 'internet': 455, 'tv': 922, 'client': 157, 'oh': 587, 'gone': 378, 'cook': 186, 'hoarder': 421, 'organ': 600, 'game': 365, 'god': 375, 'choic': 148}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer creates a vocabulary. Checking Vocabulary\n",
    "print('COUNT VECTORIZER')\n",
    "print('VOCABULARY SIZE: ', len(count_vector.vocabulary_))\n",
    "print('VOCABULARY CONTENT: ', count_vector.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# characters level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(max_df=0.90, min_df=2, analyzer='word', preprocessor=lambda x: x, tokenizer=lambda x: x,\n",
    "                                   ngram_range=(1,2), max_features=1000)\n",
    "tfidf_vect_ngram.fit(X_train)\n",
    "\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train) \n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF IDF VECTORIZER\n",
      "VOCABULARY SIZE:  1000\n",
      "VOCABULARY CONTENT:  {'food': 329, 'suppli': 864, 'consum': 159, 'depart': 222, 'govern': 381, 'take': 874, 'effect': 260, 'step': 836, 'prevent': 665, 'market': 532, 'hoard': 423, 'protect': 679, 'mask': 533, 'hand': 397, 'sanit': 750, 'state': 831, 'food suppli': 337, 'hand sanit': 398, 'group': 388, 'want': 956, 'insur': 456, 'compani': 150, 'help': 411, 'restaur': 728, 'industri': 447, 'taken': 876, 'massiv': 536, 'hit': 422, 'covid': 188, 'crisi': 198, 'covid crisi': 190, 'pandem': 611, 'bank': 63, 'day': 209, 'payment': 626, 'period': 630, 'need': 569, 'due': 251, 'list': 498, 'credit': 197, 'would': 991, 'thought': 891, 'look': 512, 'like': 494, 'groceri': 385, 'store': 845, 'rather': 694, 'go': 368, 'purchas': 682, 'thing': 888, 'look like': 513, 'groceri store': 387, 'wonder': 982, 'today': 897, 'watch': 963, 'supermarket': 859, 'fight': 315, 'old': 592, 'ladi': 474, 'last': 477, 'roll': 741, 'toilet': 899, 'paper': 616, 'go supermarket': 371, 'toilet paper': 900, 'let': 490, 'peopl': 627, 'self': 766, 'isol': 459, 'get': 361, 'essenti': 280, 'stop': 843, 'panic': 612, 'buy': 93, 'vulner': 949, 'think': 889, 'peopl need': 628, 'self isol': 767, 'stop panic': 844, 'panic buy': 613, 'ye': 994, 'stock': 838, 'forget': 339, 'stock food': 839, 'run': 744, 'low': 519, 'via': 942, 'shop': 790, 'hour': 428, 'creat': 196, 'elderli': 263, 'custom': 204, 'also': 26, 'includ': 441, 'nh': 574, 'amp': 35, 'support': 866, 'worker': 987, 'cheap': 123, 'deal': 210, 'us': 935, 'stay': 833, 'home': 425, 'activ': 8, 'feed': 313, 'chicken': 126, 'make': 524, 'follow': 328, 'email': 265, 'start': 830, 'pm': 648, 'bottl': 84, 'got': 378, 'stay home': 834, 'chain': 117, 'sure': 868, 'older': 593, 'risk': 739, 'american': 30, 'avoid': 54, 'crowd': 200, 'special': 822, 'supermarket chain': 860, 'make sure': 525, 'get food': 362, 'invest': 458, 'person': 631, 'end': 271, 'quickli': 689, 'say': 755, 'deserv': 223, 'time': 895, 'nurs': 583, 'hospit': 427, 'staff': 828, 'mani': 528, 'other': 603, 'live': 502, 'store worker': 849, 'announc': 36, 'shut': 796, 'sentiment': 774, 'post': 655, 'biggest': 78, 'drop': 249, 'record': 705, 'earli': 254, 'april': 44, 'free': 342, 'fall': 304, 'confid': 155, 'wors': 990, 'expect': 292, 'infect': 448, 'death': 211, 'rate': 693, 'soon': 818, 'allow': 23, 'economi': 259, 'import': 440, 'messag': 548, 'one': 594, 'anoth': 37, 'good': 377, 'away': 56, 'survey': 870, 'eat': 257, 'fruit': 349, 'veget': 940, 'consid': 158, 'place': 639, 'someth': 817, 'food amp': 330, 'coronaviru': 176, 'came': 99, 'light': 493, 'new': 571, 'wear': 966, 'cloth': 140, 'face': 300, 'cover': 187, 'slow': 806, 'spread': 825, 'public': 681, 'set': 779, 'social': 811, 'distanc': 237, 'measur': 541, 'difficult': 230, 'maintain': 522, 'e': 253, 'pharmaci': 633, 'social distanc': 812, 'bring': 90, 'back': 59, 'ration': 695, 'year': 995, 'pretti': 664, 'nice': 575, 'item': 462, 'packag': 609, 'food item': 333, 'cut': 205, 'crude': 201, 'price': 666, 'demand': 220, 'small': 807, 'busi': 92, 'retail': 732, 'ever': 285, 'famili': 305, 'servic': 778, 'small busi': 808, 'visit': 947, 'local': 504, 'view': 944, 'healthcar': 406, 'may': 537, 'die': 228, 'pleas': 643, 'hero': 412, 'stockpil': 842, 'local supermarket': 506, 'hike': 419, 'understand': 929, 'given': 365, 'temporari': 881, 'know': 473, 'fix': 326, 'onlin': 595, 'woman': 981, 'surviv': 871, 'lower': 520, 'ga': 355, 'issu': 460, 'account': 4, 'year old': 996, 'ga price': 356, 'health': 404, 'care': 107, 'cost': 181, 'worri': 989, 'track': 911, 'releas': 713, 'week': 970, 'said': 748, 'abl': 0, 'afford': 15, 'treatment': 916, 'health care': 405, 'last week': 478, 'rememb': 716, 'keep': 466, 'shame': 781, 'guy': 394, 'school': 759, 'meal': 539, 'guidanc': 393, 'plan': 640, 'report': 719, 'first': 325, 'launch': 482, 'march': 531, 'immedi': 437, 'global': 366, 'impact': 438, 'posit': 653, 'action': 7, 'seen': 765, 'across': 5, 'key': 467, 'sector': 761, 'base': 65, 'u': 926, 'growth': 390, 'spend': 823, 'huge': 433, 'come': 144, 'fill': 318, 'give': 364, 'employe': 268, 'thank': 887, 'corona': 174, 'behavior': 70, 'covid impact': 191, 'onlin shop': 597, 'tomorrow': 903, 'minut': 554, 'answer': 38, 'question': 686, 'travel': 914, 'household': 431, 'clear': 134, 'inform': 452, 'best': 75, 'next': 573, 'outbreak': 604, 'work': 983, 'expert': 294, 'empti': 269, 'bread': 88, 'aisl': 20, 'reason': 701, 'suppli chain': 865, 'still': 837, 'see': 763, 'hear': 408, 'way': 965, 'togeth': 898, 'apart': 41, 'whether': 975, 'outsid': 605, 'mani peopl': 529, 'fight covid': 316, 'world': 988, 'shelv': 784, 'supermarket shelv': 861, 'key worker': 468, 'case': 110, 'test': 884, 'kit': 472, 'made': 521, 'avail': 53, 'sale': 749, 'govt': 383, 'lockdown': 509, 'citi': 130, 'done': 242, 'ask': 48, 'friend': 345, 'tri': 918, 'usual': 937, 'water': 964, 'amid': 31, 'member': 547, 'use': 936, 'platform': 641, 'provid': 680, 'commun': 149, 'learn': 485, 'deliveri': 216, 'area': 45, 'amid covid': 32, 'covid pandem': 193, 'local groceri': 505, 'love': 518, 'littl': 501, 'card': 106, 'futur': 354, 'info': 451, 'shop onlin': 791, 'viru': 946, 'situat': 803, 'increas': 443, 'soap': 809, 'increas price': 445, 'oil': 589, 'around': 46, 'continu': 169, 'gallon': 357, 'surg': 869, 'firm': 324, 'forc': 338, 'chang': 120, 'habit': 395, 'covid outbreak': 192, 'chang consum': 121, 'farmer': 308, 'produc': 674, 'much': 563, 'respect': 724, 'went': 973, 'morn': 561, 'awar': 55, 'reduc': 706, 'cash': 111, 'went groceri': 974, 'price drop': 667, 'disinfect': 235, 'necessari': 568, 'kill': 470, 'call': 97, 'clean': 133, 'rs': 742, 'till': 894, 'c': 96, 'detail': 226, 'click': 136, 'absolut': 1, 'serious': 776, 'prepar': 661, 'media': 543, 'fear': 310, 'rice': 735, 'india': 446, 'countri': 185, 'rise': 738, 'number': 582, 'respond': 725, 'closur': 139, 'shock': 789, 'ensur': 276, 'eye': 299, 'open': 598, 'experi': 293, 'possibl': 654, 'w': 950, 'hard': 402, 'find': 322, 'put': 684, 'car': 105, 'trolley': 920, 'full': 352, 'left': 488, 'alreadi': 25, 'total': 906, 'product': 675, 'feel': 314, 'hold': 424, 'partner': 620, 'resourc': 723, 'anyon': 39, 'great': 384, 'k': 465, 'safe': 746, 'fed': 311, 'queue': 687, 'buyer': 95, 'tell': 880, 'scare': 758, 'happen': 400, 'though': 890, 'mom': 556, 'believ': 73, 'could': 183, 'mean': 540, 'life': 492, 'frontlin': 348, 'often': 587, 'accord': 3, 'data': 207, 'onlin groceri': 596, 'driver': 248, 'store employe': 847, 'deliveri driver': 217, 'design': 224, 'webinar': 968, 'relat': 712, 'regist': 709, 'trend': 917, 'consum behavior': 160, 'research': 721, 'everyon': 288, 'front': 346, 'line': 496, 'appreci': 43, 'front line': 347, 'sinc': 800, 'babi': 58, 'go shop': 370, 'sick': 797, 'cough': 182, 'quarantin': 685, 'order': 602, 'ago': 17, 'slot': 805, 'execut': 291, 'deliveri slot': 219, 'egg': 262, 'without': 980, 'kind': 471, 'stuff': 856, 'drive': 247, 'enter': 277, 'milk': 550, 'food bank': 331, 'gener': 360, 'collaps': 142, 'th': 886, 'drug': 250, 'charg': 122, 'oil price': 590, 'latest': 481, 'everi': 286, 'singl': 801, 'saw': 754, 'pay': 625, 'play': 642, 'rule': 743, 'supermarket staff': 862, 'doctor': 239, 'job': 463, 'employ': 267, 'spread covid': 826, 'go groceri': 369, 'uk': 927, 'offic': 585, 'goug': 379, 'scam': 756, 'price goug': 669, 'soar': 810, 'news': 572, 'demand food': 221, 'catch': 113, 'sever': 780, 'discuss': 233, 'potenti': 656, 'read': 697, 'coronaviru covid': 177, 'meat': 542, 'wast': 962, 'bag': 61, 'ban': 62, 'shopper': 792, 'note': 579, 'man': 526, 'oper': 599, 'send': 771, 'receiv': 702, 'bill': 79, 'heart': 410, 'goe': 373, 'critic': 199, 'shift': 786, 'relief': 714, 'addit': 11, 'limit': 495, 'park': 618, 'ticket': 893, 'late': 479, 'must': 564, 'manag': 527, 'lot': 517, 'groceri shop': 386, 'door': 244, 'almost': 24, 'month': 560, 'carri': 108, 'truck': 921, 'work home': 985, 'stay safe': 835, 'pressur': 663, 'ventil': 941, 'realli': 700, 'manufactur': 530, 'war': 957, 'effort': 261, 'consum good': 164, 'serv': 777, 'children': 127, 'face mask': 301, 'wash': 960, 'better': 76, 'wash hand': 961, 'supermarket worker': 863, 'phone': 634, 'social media': 813, 'china': 128, 'disrupt': 236, 'scammer': 757, 'advantag': 12, 'feder': 312, 'trade': 912, 'commiss': 146, 'take advantag': 875, 'polic': 650, 'slash': 804, 'econom': 258, 'australia': 51, 'per': 629, 'close': 138, 'liter': 500, 'b': 57, 'wage': 951, 'due covid': 252, 'becom': 68, 'highlight': 418, 'differ': 229, 'concern': 153, 'push': 683, 'move': 562, 'major': 523, 'shortag': 794, 'empti shelv': 270, 'well': 972, 'yet': 998, 'imagin': 436, 'gov': 380, 'street': 851, 'safeti': 747, 'reach': 696, 'connect': 157, 'ppe': 658, 'ship': 787, 'restrict': 730, 'sell': 769, 'deliv': 215, 'less': 489, 'basic': 66, 'explain': 295, 'retail store': 733, 'donat': 241, 'contact': 167, 'power': 657, 'big': 77, 'temporarili': 882, 'regul': 710, 'consum protect': 165, 'leav': 487, 'bare': 64, 'affect': 14, 'fresh': 343, 'even': 283, 'told': 902, 'recent': 703, 'right': 737, 'distribut': 238, 'pasta': 623, 'box': 86, 'societi': 814, 'extra': 297, 'program': 678, 'lead': 484, 'glove': 367, 'near': 567, 'term': 883, 'meet': 546, 'half': 396, 'sign': 799, 'citizen': 131, 'updat': 933, 'alert': 22, 'top': 905, 'might': 549, 'date': 208, 'book': 83, 'check': 124, 'bit': 81, 'alway': 27, 'remain': 715, 'chines': 129, 'nation': 566, 'paid': 610, 'normal': 578, 'medic': 544, 'tp': 910, 'farm': 307, 'past': 622, 'selfish': 768, 'share': 782, 'common': 148, 'financi': 321, 'link': 497, 'tip': 896, 'challeng': 118, 'part': 619, 'develop': 227, 'cannot': 103, 'vaccin': 938, 'co': 141, 'grow': 389, 'insight': 454, 'act': 6, 'blog': 82, 'impact covid': 439, 'consum confid': 162, 'refus': 707, 'rent': 718, 'owner': 606, 'contract': 170, 'respons': 726, 'access': 2, 'volunt': 948, 'easter': 256, 'readi': 698, 'show': 795, 'save': 753, 'miss': 555, 'itali': 461, 'collect': 143, 'presid': 662, 'associ': 50, 'restock': 729, 'unit': 931, 'agricultur': 19, 'websit': 969, 'sanitis': 751, 'amidst': 33, 'complaint': 151, 'daili': 206, 'caus': 114, 'canadian': 101, 'pick': 635, 'least': 486, 'instead': 455, 'yesterday': 997, 'poor': 652, 'someon': 816, 'commerc': 145, 'mind': 552, 'els': 264, 'declin': 214, 'found': 340, 'event': 284, 'offici': 586, 'requir': 720, 'pet': 632, 'trip': 919, 'talk': 877, 'contain': 168, 'pile': 638, 'food shop': 335, 'sourc': 819, 'fact': 302, 'consum spend': 166, 'folk': 327, 'bad': 60, 'crash': 194, 'south': 820, 'minist': 553, 'fund': 353, 'idea': 435, 'expos': 296, 'test posit': 885, 'recess': 704, 'plu': 645, 'etc': 282, 'amazon': 28, 'wait': 952, 'long': 510, 'studi': 855, 'exampl': 290, 'stress': 852, 'wish': 979, 'touch': 907, 'rais': 691, 'notic': 581, 'crazi': 195, 'locat': 507, 'condit': 154, 'later': 480, 'offer': 584, 'lost': 516, 'town': 909, 'turn': 923, 'money': 559, 'enough': 274, 'incom': 442, 'walk': 954, 'saudi': 752, 'white': 976, 'pack': 608, 'control': 171, 'short': 793, 'current': 203, 'prioriti': 670, 'store close': 846, 'side': 798, 'join': 464, 'assist': 49, 'increas demand': 444, 'regular': 711, 'ridicul': 736, 'actual': 9, 'corona viru': 175, 'healthi': 407, 'hey': 413, 'non': 577, 'final': 319, 'st': 827, 'station': 832, 'guess': 392, 'seem': 764, 'dollar': 240, 'cancel': 102, 'advic': 13, 'fail': 303, 'everyth': 289, 'file': 317, 'pictur': 636, 'night': 576, 'calm': 98, 'amount': 34, 'york': 999, 'stori': 850, 'loss': 515, 'anyth': 40, 'medicin': 545, 'privat': 671, 'hope': 426, 'fine': 323, 'essenti item': 281, 'moment': 557, 'work groceri': 984, 'confirm': 156, 'respons covid': 727, 'remind': 717, 'cart': 109, 'canada': 100, 'inflat': 449, 'break': 89, 'n': 565, 'inflat price': 450, 'second': 760, 'probabl': 672, 'option': 601, 'practic': 660, 'ppl': 659, 'noth': 580, 'human': 434, 'toilet roll': 901, 'suggest': 858, 'never': 570, 'fast': 309, 'walmart': 955, 'high': 415, 'high price': 416, 'build': 91, 'cashier': 112, 'sold': 815, 'monday': 558, 'space': 821, 'coupl': 186, 'longer': 511, 'claim': 132, 'point': 649, 'higher': 417, 'emerg': 266, 'price fall': 668, 'enough food': 275, 'checkout': 125, 'work supermarket': 986, 'p': 607, 'regard': 708, 'interest': 457, 'piec': 637, 'counti': 184, 'pantri': 615, 'food price': 334, 'dont': 243, 'fuck': 350, 'despit': 225, 'law': 483, 'trump': 922, 'ad': 10, 'hous': 429, 'hous price': 430, 'profit': 677, 'panic buyer': 614, 'direct': 232, 'debt': 212, 'clerk': 135, 'diseas': 234, 'especi': 279, 'equip': 278, 'extrem': 298, 'articl': 47, 'problem': 673, 'friday': 344, 'basket': 67, 'shelf': 783, 'food deliveri': 332, 'deliveri servic': 218, 'mayb': 538, 'secur': 762, 'head': 403, 'polici': 651, 'insid': 453, 'return': 734, 'two': 924, 'patient': 624, 'video': 943, 'food stock': 336, 'bought': 85, 'larg': 475, 'virtual': 945, 'urg': 934, 'profession': 676, 'kid': 469, 'senior': 772, 'real': 699, 'behind': 72, 'cure': 202, 'enforc': 273, 'buy food': 94, 'spike': 824, 'took': 904, 'r': 690, 'listen': 499, 'symptom': 872, 'complet': 152, 'wrong': 992, 'covid consum': 189, 'store shelv': 848, 'ceo': 116, 'team': 879, 'warehous': 958, 'happi': 401, 'america': 29, 'suffer': 857, 'level': 491, 'financ': 320, 'tax': 878, 'pass': 621, 'struggl': 854, 'parent': 617, 'million': 551, 'capit': 104, 'author': 52, 'plenti': 644, 'hire': 421, 'wear mask': 967, 'seriou': 775, 'cent': 115, 'stock price': 841, 'wipe': 978, 'site': 802, 'warn': 959, 'commod': 147, 'hike price': 420, 'agre': 18, 'plummet': 646, 'transport': 913, 'rais price': 692, 'mass': 535, 'shit': 788, 'app': 42, 'sent': 773, 'result': 731, 'far': 306, 'three': 892, 'consum demand': 163, 'gold': 374, 'lose': 514, 'decid': 213, 'treat': 915, 'system': 873, 'gonna': 376, 'begin': 69, 'strong': 853, 'seller': 770, 'age': 16, 'stand': 829, 'shelv empti': 785, 'governor': 382, 'cope': 173, 'behaviour': 71, 'heard': 409, 'unpreced': 932, 'russia': 745, 'whole': 977, 'largest': 476, 'stock market': 840, 'everi day': 287, 'easi': 255, 'toward': 908, 'energi': 272, 'dr': 246, 'hi': 414, 'unemploy': 930, 'howev': 432, 'chanc': 119, 'fuel': 351, 'gather': 359, 'opportun': 600, 'brand': 87, 'quick': 688, 'four': 341, 'ok': 591, 'benefit': 74, 'lock': 508, 'road': 740, 'corpor': 180, 'mask glove': 534, 'coronaviru pandem': 179, 'unabl': 928, 'resid': 722, 'handl': 399, 'giant': 363, 'suppos': 867, 'two week': 925, 'weekli': 971, 'loan': 503, 'alcohol': 21, 'doubl': 245, 'digit': 231, 'gt': 391, 'billion': 80, 'x': 993, 'wake': 953, 'plung': 647, 'valu': 939, 'client': 137, 'oh': 588, 'gone': 375, 'cook': 172, 'consum behaviour': 161, 'game': 358, 'coronaviru outbreak': 178, 'god': 372}\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer also creates a vocabulary. Checking Vocabulary\n",
    "print('TF IDF VECTORIZER')\n",
    "print('VOCABULARY SIZE: ', len(tfidf_vect_ngram.vocabulary_))\n",
    "print('VOCABULARY CONTENT: ', tfidf_vect_ngram.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train_bow = count_vector.transform(X_train).toarray()\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_bow = count_vector.transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize BoW features in training and test set\n",
    "bow_normalizer = pr.Normalizer().fit(X_train_bow)\n",
    "tfidf_normalizer = pr.Normalizer().fit(X_train_tfidf_ngram)\n",
    "\n",
    "X_train_bow = bow_normalizer.transform(X_train_bow)\n",
    "X_test_bow  = bow_normalizer.transform(X_test_bow)\n",
    "\n",
    "X_train_tfidf_ngram = bow_normalizer.transform(X_train_tfidf_ngram)\n",
    "X_test_tfidf_ngram = bow_normalizer.transform(X_test_tfidf_ngram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "\n",
    "- Gradient Boosting\n",
    "- Random Forest\n",
    "- K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT BOOSTING FOR COUNT VECTORIZER\n",
      "0.7467644521138913\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT BOOSTING FOR COUNT VECTORIZER\n",
    "xgboost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train_bow, y_train)\n",
    "\n",
    "print('GRADIENT BOOSTING FOR COUNT VECTORIZER')\n",
    "print(xgboost.score(X_test_bow, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT BOOSTING FOR TF-IDF\n",
      "0.7391429393155019\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT BOOSTING FOR TF-IDF\n",
    "xgboost_tf = GradientBoostingClassifier(n_estimators=400, learning_rate=.5,\n",
    "    max_depth=1, random_state=13).fit(X_train_tfidf_ngram, y_train)\n",
    "\n",
    "print('GRADIENT BOOSTING FOR TF-IDF')\n",
    "print(xgboost_tf.score(X_test_tfidf_ngram, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST FOR COUNT VECTORIZER\n",
      "0.6199309749784296\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST FOR COUNT VECTORIZER\n",
    "Ran_For = RandomForestClassifier(max_depth=2, n_estimators=100, criterion='entropy')\n",
    "Ran_For.fit(X_train_bow, y_train)\n",
    "\n",
    "print('RANDOM FOREST FOR COUNT VECTORIZER')\n",
    "print(Ran_For.score(X_test_bow, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST FOR TF-IDF\n",
      "0.6084268047167098\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST FOR TF-IDF\n",
    "Ran_For_tf = RandomForestClassifier(max_depth=2, n_estimators=100, criterion='entropy')\n",
    "Ran_For_tf.fit(X_train_tfidf_ngram, y_train)\n",
    "\n",
    "print('RANDOM FOREST FOR TF-IDF')\n",
    "print(Ran_For_tf.score(X_test_tfidf_ngram, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN FOR COUNT VECTORIZER\n",
      "0.41141788898475695\n"
     ]
    }
   ],
   "source": [
    "# KNN FOR COUNT VECTORIZER\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train_bow, y_train)\n",
    "\n",
    "print('KNN FOR COUNT VECTORIZER')\n",
    "print(neigh.score(X_test_bow, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test    - [0] \n",
      "\n",
      "XGB  CV - [1]\n",
      "RaFo CV - [1]\n",
      "K NN CV - [0]\n",
      "XGB  TF - [1]\n",
      "RaFo TF - [1]\n",
      "K NN TF - [0]\n"
     ]
    }
   ],
   "source": [
    "sel = 6\n",
    "print('Test    -',y_test[sel:sel+1],'\\n')\n",
    "\n",
    "print('XGB  CV -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('RaFo CV -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('K NN CV -',neigh.predict(X_test_bow[sel:sel+1]))\n",
    "\n",
    "print('XGB  TF -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('RaFo TF -',Ran_For.predict(X_test_bow[sel:sel+1]))\n",
    "print('K NN TF -',neigh.predict(X_test_bow[sel:sel+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_models/CV_covid.pkl\n",
      "./saved_models/xgboost_covid.pkl\n",
      "./saved_models/TF_covid.pkl\n",
      "./saved_models/CV_normalizer.pkl\n",
      "./saved_models/TF_normalizer_1-2gram.pkl\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "model_data = {'model': xgboost, 'filename':'./saved_models/xgboost_covid.pkl'}\n",
    "CV_data = {'model': count_vector, 'filename': './saved_models/CV_covid.pkl'}\n",
    "TF_data = {'model': tfidf_vect_ngram, 'filename': './saved_models/TF_covid.pkl'}\n",
    "CV_normalizer = {'model':bow_normalizer, 'filename': './saved_models/CV_normalizer.pkl'}\n",
    "TF_normalizer = {'model':tfidf_normalizer, 'filename': './saved_models/TF_normalizer_1-2gram.pkl'}\n",
    "\n",
    "package = [CV_data,model_data, TF_data, CV_normalizer, TF_normalizer]\n",
    "\n",
    "for i in package:\n",
    "    with open(i['filename'], 'wb') as f:\n",
    "        cloudpickle.dump(i['model'], f)\n",
    "    print(i['filename'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "#loaded_model = cloudpickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
